# NewFeatures: Paper2Poster Parser Pipeline - Mistral OCR Integration PRD(1)

issue tag: #NewFeatures #PDF2MD-Pipeline

Integrating Mistral OCR into Paper2Poster Parser Pipeline

Background and Context

The Paper2Poster project currently uses PDF parsing tools (notably the open-source Marker library and Docling) to convert scientific paper PDFs into structured Markdown and JSON assets  . This parser stage (“Parser agent”) distills a paper into a structured asset library – including text sections, figures, and tables – which downstream agents use for poster planning and generation. Marker has provided fast PDF-to-Markdown conversion with image extraction , while Docling offers advanced document understanding capabilities . However, Marker’s continued use has downsides (external dependency, maintenance, and possibly limitations in accuracy or cost), and Mistral AI’s Document OCR has emerged as a superior alternative.

Mistral OCR is a state-of-the-art document understanding API that uses an LLM-based model (mistral-ocr-latest) to extract text, images, tables, and structure from PDFs with high accuracy  . It preserves document hierarchy and returns results in Markdown format with embedded images and table structures . Mistral’s API can handle up to ~1000 pages per document (50MB limit) in a single call , and supports batch processing for very large documents . By integrating Mistral OCR as the primary parser, we aim to improve the accuracy and reliability of content extraction (especially for complex layouts, math, and multi-column text ) while simplifying the codebase by removing Marker. Docling will be retained as a fallback to ensure robustness if Mistral fails or produces incomplete results.

Objective

Integrate Mistral OCR as the primary PDF-to-Markdown converter in the Paper2Poster parsing pipeline, replacing Marker, and implement Docling as a fallback. The new solution must maintain compatibility with the existing output format (content and asset JSON files) so that downstream components (planning and poster generation) continue to work seamlessly. Additionally, it should include robust error handling, support for large documents, and adhere to best practices for clean, maintainable, and testable Python code.

Requirements and Scope

The solution will meet the following requirements:
	1.	Mistral OCR as Primary Converter: Use the Mistral Document AI OCR API (mistral-ocr-latest) to convert PDFs into structured Markdown with extracted images and tables. The parser should call Mistral’s API (via official Python SDK or HTTP) to get the document content. The API returns markdown text preserving headings, lists, and table structures, along with extracted images in base64 . This integration should handle both research papers (multi-page PDFs) and other supported formats if needed.
	2.	Remove Marker Dependency: Eliminate all usage of the Marker tool in the repository. This includes removing any marker library calls, CLI usage, or related configuration. The code should no longer rely on Marker for PDF parsing. Any references to Marker’s output format need to be refactored to use Mistral’s output. We will update requirements.txt or environment setup to drop Marker and add Mistral’s SDK (mistralai).
	3.	Docling as Fallback: Implement Docling as a fallback parser if Mistral’s API call fails (e.g., network error, API error) or if the returned content is insufficient (e.g. incomplete text or missing critical elements like figures/tables). Docling is an open-source library that can parse PDFs into a unified structure with markdown, JSON, and images . If triggered, the fallback should use Docling to process the PDF and produce the required outputs. This ensures robust parsing even if Mistral has outages or cannot handle a particular document.
	4.	Preserve Output Data Structure: The output of the parser must remain fully compatible with the existing data structures expected by downstream components:
	•	_raw_content.json: A JSON containing the structured paper content by sections (with section titles, possibly summaries, and body text). The exact structure used in Paper2Poster should be preserved. For example, if currently each top-level section (Abstract, Introduction, etc.) is an entry with a summary and content, the same format must be output.
	•	_images.json: A list of figures/images extracted from the paper, each with an id (e.g., “Figure 1”), caption text, image size or dimensions, and a base64-encoded image data. The new pipeline should populate this list with all figures that Mistral OCR extracts (using the base64 images returned by Mistral ) or from Docling fallback. Ensure the figure numbering/captions are correctly associated.
	•	_tables.json: A list of tables extracted from the paper, each with an id (e.g., “Table 1”), caption, size/dimensions, and base64 image or representation. Mistral OCR represents tables in markdown (preserving rows/columns as text) . To maintain consistency with the current pipeline, we may need to capture tables as images or structured data in the JSON. If the existing pipeline expects tables as images, implement a step to convert the markdown table to an image (or use Docling’s output if it provides a table image or HTML). At minimum, include table captions and either the markdown or an image of the table in _tables.json so that the Planner can handle tables similar to figures.
	•	All IDs, file naming, and JSON schema should remain unchanged so that no other module breaks. This likely means writing a converter that takes Mistral’s raw output (likely a JSON with page-wise content and images) and transforms it into the above three JSON files in the same format as before.
	5.	API Key Configuration & Error Handling: Provide a configurable way to supply the Mistral API key (e.g., via environment variable like MISTRAL_API_KEY, or a config file entry). The code should read the API key securely and initialize the Mistral client with it . If the key is not provided or invalid, the parser should log an error and either fail gracefully or fall back to Docling automatically. Include robust error handling around the Mistral API call: catch network timeouts, HTTP errors, or API exceptions. In case of failure, log the issue with details (but avoid exposing sensitive info like the key) and trigger the fallback to ensure the pipeline continues. Additionally, implement reasonable retry logic for transient errors (e.g., if Mistral call fails due to a network glitch, maybe retry once or twice before falling back).
	6.	Fallback Trigger Logic: Design and implement logic to detect when to use the fallback. Beyond catching explicit errors, we must identify “insufficient output” scenarios, for example:
	•	Incomplete Markdown Content: If the Mistral output is truncated or missing sections. We can compare the number of pages or length of text returned against the PDF’s properties. For instance, if a PDF has 10 pages but Mistral returns content for fewer pages (e.g., only 8 pages of content), that suggests a possible truncation. The code can check if the number of page segments in Mistral’s result matches the PDF page count (if available via PDF metadata or Mistral’s JSON).
	•	Missing Figures/Tables: If the PDF is known to contain figures or tables (perhaps by scanning for the words “Figure” or “Table” in the text, or by PDF metadata) but _images.json or _tables.json would end up empty from Mistral output, it may indicate Mistral failed to extract them. In such cases, trigger Docling to fill in the gaps (either replacing the whole content or just extracting the missing parts). Another approach: check the Markdown for placeholders like ![img-...](...) which denote images. If no such references are present but Docling or a quick PDF check indicates images exist, treat that as a gap.
	•	Token Length or Content Gaps: If the extracted text is significantly shorter than expected (e.g., a 30-page paper resulting in only a few hundred words of markdown), that’s a red flag. We might implement a heuristic: if the total token count or character count of Mistral’s output is below a threshold relative to PDF length (like less than X characters per page on average), trigger fallback. Also, if key sections (e.g., “Introduction” or “References”) are completely missing from the markdown (when they should likely be present in an academic paper), that indicates a possible parsing failure.
The fallback trigger logic should be well-documented and adjustable if needed. It’s important to avoid false positives (don’t fallback just because a short paper legitimately has few images, for example). We will likely combine a few heuristics (page count mismatch, missing images, etc.) to decide. When a fallback is used, clearly log that Mistral output was deemed incomplete and we are switching to Docling.
	7.	Handle Large PDFs (>1000 pages): Mistral’s single-call limit is ~1000 pages . For documents longer than this (or very large file sizes), implement batch processing. This could be done by splitting the PDF into smaller chunks and calling the OCR API on each chunk sequentially (or using Mistral’s batch inference capabilities if available to send multiple chunks in parallel). The logic should: detect if page count exceeds 1000 (we can use a PDF library or Mistral might reject such a call), then split the PDF (e.g., into 500-page parts or whatever is optimal) and process each part. After processing, we need to merge the results: concatenate the markdown content in the correct order, and merge the images and tables lists (ensuring figure/table IDs remain unique and sequential). This merging should preserve the structure (e.g., if each chunk is processed separately, section headings that span chunks should be handled carefully). Alternatively, if Mistral API/SDK supports a built-in batch mode to handle large docs (possibly it allows sending the PDF as multiple parts in one request), we can utilize that – but if not, manual splitting is fine. Ensure that after merging, the output JSON still looks as if one continuous document was processed. Add tests for a simulated large PDF scenario if possible.
	8.	Code Quality and Maintainability: All changes should follow best practices for clean, maintainable, and testable Python code. This includes:
	•	Refactoring or organizing the parser code (perhaps in a module like doc_converter.py or similar) so that the logic for Mistral integration and fallback is clear and modular. For instance, one could implement a function convert_pdf_to_markdown(pdf_path) that encapsulates “call Mistral, validate output, maybe fallback with Docling, return structured data”.
	•	Use descriptive function and variable names, and include docstrings for new functions/classes explaining their purpose.
	•	Avoid hard-coding values; use configuration or constants for things like API endpoints, page size limits, etc.
	•	Ensure any new external dependencies (like Mistral’s SDK or additional PDF libraries) are added to requirements. Remove Marker from requirements to prevent confusion.
	•	Write unit tests for the new functionality (e.g., a test for the fallback trigger logic given various artificial outputs, a test for merging chunked outputs, etc.). Where direct API calls are involved, consider mocking the Mistral API response for testing to avoid relying on the external service.
	•	Keep the code style consistent with the project (follow PEP8, use type hints if the project uses them, etc.).
	•	Document any significant assumptions or decisions in comments or the project documentation. For example, if we decide to convert tables to images via a specific method, document why and how.

Out of Scope: This integration focuses on the parser stage only. We do not modify the Planner, Painter, or other downstream logic except to ensure they continue to receive data in the format they expect. Also, optimizing the Mistral OCR model itself or altering the content summarization logic is out of scope – we assume the LLM summarization (if any, for section summaries) will work with the new content as long as format is consistent. Any performance tuning (beyond handling large PDFs) is limited to ensuring the new pipeline runs within reasonable time for typical inputs.

Proposed Solution Design

Mistral OCR Integration (doc_converter Module)

We will implement a new doc converter module (e.g., utils/doc_converter_mistral.py or similar) that interfaces with the Mistral API. Using Mistral’s Python SDK, the code will load the PDF (as a local file or base64) and call client.ocr.process(model="mistral-ocr-latest", document={...}, include_image_base64=True)  . The response will include the recognized content in Markdown plus image data. We will parse that response: likely the SDK returns a structured object or dict. For example, each page might be a dict with markdown text and an images list containing base64 strings . We’ll need to iterate over pages to build the full paper content and aggregate images and tables. The integration details:
	•	Dependency: Add mistralai (the Mistral SDK) to the project. Read API key from env or config and initialize the client.
	•	Conversion Call: Implement convert_with_mistral(pdf_path) that handles the API call. It should open/read the PDF (or just provide the file path to the SDK if supported), and handle any exceptions (invalid file, API error). If conversion succeeds, it returns a result object (or our own intermediate representation). If an exception occurs, log it and return a special status indicating failure.
	•	Result Parsing: Process Mistral’s output to create the _raw_content.json, _images.json, _tables.json. This will likely involve:
	•	Combining page-wise markdown into one structured markdown (or directly into JSON by splitting sections). Possibly, we might feed the combined markdown into the existing logic that splits into sections. However, since the original pipeline used Marker/Docling JSON directly, it might be easier to directly construct the JSON. We can use cues like markdown headings (#, ##) to identify sections, or rely on Mistral’s understanding of hierarchy (it might output proper heading levels in markdown corresponding to sections/subsections).
	•	Extracting figures: Scan the markdown for image tags ![img-X](img-X) which Mistral uses as placeholders . For each image tag, find the corresponding image data in the response (Mistral’s JSON has an images list with id matching the filename and a base64 string ). Create an entry in _images.json with id (maybe use the Figure number if caption text includes “Figure 1:”, etc.), caption (the text immediately following the image in markdown, e.g. the line starting “Figure 1:”), and the base64. The size/dimensions can be extracted from the image’s metadata (top_left_x, top_left_y, bottom_right_x, bottom_right_y can give width/height in pixels , or use the provided dimensions.dpi and page size to calculate real size).
	•	Extracting tables: Identify tables in Mistral’s output. Mistral might output tables as GitHub-flavored markdown tables (a series of | and - lines) . We need to find where these occur. Also, table captions might appear as text (e.g., “Table 1: …”). If Mistral treats tables similar to figures, it might output an image for a table if it was detected as a figure – but more likely it leaves it as text. We have two options:
	1.	Keep as text: We could include table content as markdown in the _raw_content.json and also create a _tables.json entry that contains the table in a textual form (perhaps as HTML or markdown) along with its caption. But if the poster generator expects an image, this might not directly work.
	2.	Convert to image: Use an approach to render the markdown table to an image. This could be done by leveraging a headless browser or a Python library (like matplotlib for simple tables or PIL). Given complexity, an alternative is to use Docling (if Docling can output a snapshot of the table or at least identify table region in PDF to extract as image). Docling’s advanced PDF analysis might let us get a table’s bounding box and image. If available, we can integrate that: e.g., if Mistral outputs a table as text, we could cross-check with Docling’s parsed output to get an image of that table region. This is a stretch goal; to start, we might include just the textual representation.
In any case, ensure _tables.json is populated with table captions (e.g., “Table 2: Experimental results…”) and some representation of the table.
	•	Section Summaries: The requirement mentions “summaries” in _raw_content.json. If previously the pipeline had LLM-generated summaries for each section, we should ensure not to remove that functionality. Possibly, after extracting raw content, the parser agent or another step queries an LLM to summarize each section. That part can remain unchanged if we feed it the same structure. So our job is to put each section’s full text in the JSON, and the summarizer (if any) will produce the summary field. If the summaries were being produced by Marker or Docling automatically (unlikely), we might need to implement it. Most likely, Paper2Poster had a step using an LLM for summarization, which we will keep.
	•	After constructing the JSON outputs, save them in the same format and filenames as before so the Planner can pick them up.

Fallback with Docling

We will integrate Docling as the fallback mechanism:
	•	When to Fallback: As described in requirement 6, detect errors or content gaps after Mistral processing. We’ll implement a function is_output_complete(mistral_result) that returns False if any trigger condition is met (error flag, low content length, etc.). If False, we log a warning (“Mistral OCR output incomplete, using Docling fallback for parsing.”) and proceed to fallback.
	•	Docling Parsing: Docling (via its Python API or CLI) can parse the PDF. We have a docling directory in the repo, likely indicating an integration. We can call Docling’s main interface (possibly something like from docling import parse_document or via CLI docling parse file.pdf). Docling can output a DoclingDocument which we can then export to JSON/Markdown . We should configure Docling to output the needed content: ideally, it can give us a structured JSON with sections, images, and tables. If Docling doesn’t directly output our target JSON schema, we might need to adapt it (similar to how we handle Mistral’s output). Possibly, Docling can output a unified JSON which we then split into our _raw_content, _images, _tables files.
	•	Use in Code: Implement convert_with_docling(pdf_path) that returns data in the same intermediate format as convert_with_mistral. This way, after calling either, we can use a common function to build the final JSON outputs.
	•	If both Mistral and Docling fail (rare, but possible if the PDF is very problematic), ensure the error is propagated properly (the parser agent might then halt with an error message, as there’s no further fallback). Log a clear error in that case.

Output Structure Compatibility

To reiterate, the final JSON files should be indistinguishable (in schema) from those produced in the previous version of the parser. We might conduct a quick comparison on a sample PDF: run the old Marker-based pipeline and the new Mistral-based pipeline, and diff the JSON outputs to ensure all expected fields are present and correctly populated (aside from differences in content which might actually improve). Key points: section ordering should remain the same as in the PDF, figure and table numbering should match their occurrence order, and captions should be intact. If Mistral returns any extra content (like OCR’d header/footers or references that Marker used to strip ), we may need to filter those out to maintain consistency (Marker explicitly removed headers/footers ; if Mistral includes them, perhaps detect repetitive text on each page and remove). Maintain any post-processing that Marker/Docling did, such as cleaning weird characters or combining hyphenated words at line breaks, etc., either by relying on Mistral’s output quality or adding a similar cleanup pass.

Performance Considerations

Using Mistral’s API means network calls and potentially cost. We should be mindful of:
	•	API Latency: Mistral OCR is cloud-based. Parsing a large PDF might take several seconds. We should ensure our code handles this asynchronously or at least doesn’t freeze other parts. (If the architecture allows the parser agent to run independently, this might be fine.)
	•	Batch requests: If multiple PDF parsing tasks could run in parallel, be careful about rate limits or use of the API key. Possibly queue them or use batch mode.
	•	Cost: Not directly a code issue, but we should note that at 1000 pages per $1 for Mistral , the cost is low but non-zero. Removing Marker (which was local) means we incur API cost. This is accepted, but we should avoid unnecessary re-calls (e.g., caching results for the same PDF if ever needed, though likely not needed in this pipeline).
	•	Memory: Converting large PDFs might involve large data (base64 images). Ensure that reading the PDF and holding the output in memory is manageable (maybe stream pages if possible). For extremely large PDFs, our chunking approach alleviates memory by processing in parts.

Testing and Validation

We will thoroughly test the new parser pipeline:
	•	Unit Tests: Develop unit tests for critical functions:
	•	Mistral output parsing logic: feed a simulated or sample Mistral JSON (we can use a small sample from docs or a manually curated one) into our parsing function and assert that _raw_content.json, _images.json, _tables.json are correctly formed (e.g., correct number of sections, images, etc.).
	•	Fallback trigger logic: create dummy outputs (e.g., an intentionally truncated markdown) and ensure is_output_complete returns False and triggers fallback. Also test scenarios that should not trigger fallback (e.g., a normal complete output).
	•	Merging large PDF chunks: simulate two partial outputs (with images and text) and ensure the merge function produces a coherent single output with all images and pages in order.
	•	Integration Tests: Run the parser end-to-end on a few sample PDFs:
	•	A typical conference paper PDF (~8-12 pages with figures and tables). Check that the content is parsed, images and tables extracted, and no fallback is triggered (assuming Mistral handles it). Verify the resulting JSON files make sense (manually or via a known-good baseline from Marker/Docling).
	•	An edge-case PDF, e.g., very long (if available) or with unusual formatting. If we can’t find a 1000+ page doc easily, simulate by duplicating a PDF’s pages to exceed the limit, and ensure our batching works and the results include content from all pages.
	•	A PDF with known parsing difficulties (maybe a scanned PDF or one with complicated tables) – see if Mistral fails or produces partial output, and if so, confirm that our pipeline falls back to Docling and still produces output.
	•	Performance Test: If possible, measure the time for parsing a standard document vs. old pipeline to ensure it’s within acceptable range. Mistral might be slightly slower for large docs compared to local Marker, but if it’s too slow, we might need to note that or consider parallelization.
	•	Regression Test: Confirm that after integration, the Planner agent can consume _raw_content.json, etc., without errors and produce a poster layout. Essentially, run the entire Paper2Poster flow on a sample input to ensure nothing downstream broke. The poster content should be qualitatively similar (any differences in wording would come from differences in how Marker vs Mistral extract text, but it should still be coherent).

All tests should be automated where feasible (except manual inspection). Document any new test cases in the test suite.

Documentation

Update the project documentation to reflect this change:
	•	README.md: If the README previously instructed installing Marker or described the parser, update it to mention Mistral OCR integration. Explain any setup needed (e.g., obtaining a Mistral API key and setting an environment variable). Provide a brief note on fallback: e.g., “By default, Paper2Poster now uses Mistral OCR for parsing PDFs. If this fails, it automatically falls back to a local parser (Docling).”
	•	Configuration: Document how to configure the API key and any adjustable parameters (like environment vars for batch size or fallback sensitivity thresholds, if we make those configurable). Possibly create a section in README or a separate config markdown.
	•	Developer Docs: If there’s a CONTRIBUTING or developer guide, add notes on the new dependency (how to install Mistral SDK, link to Mistral API docs  for reference). Also, document the structure of _raw_content.json, _images.json, _tables.json clearly in case new contributors want to understand it – this might already exist, but ensure it’s up-to-date if anything changed.
	•	In-line Documentation: As mentioned, ensure code is commented where non-obvious. For example, comment the reasoning in the fallback logic so future maintainers understand the heuristics.

By completing these steps, the parser pipeline will use Mistral OCR as the core, yielding improved extraction of text and visuals from papers, while removing Marker (simplifying dependencies) and leveraging Docling to cover edge cases. The output remains consistent with the original format, enabling a smooth transition with no impact on the later stages of Paper2Poster.

Acceptance Criteria
	•	All PDF-to-Markdown conversion in the pipeline is done via Mistral OCR, and the Marker tool is no longer invoked anywhere in the codebase.
	•	For a set of test papers, the new parser produces _raw_content.json, _images.json, _tables.json that contain all major content (sections, figures, tables) as before. Visual inspection or automated checks confirm that no content is lost compared to the Marker-based output (differences in text formatting are acceptable, but missing figures or sections are not).
	•	When Mistral API is unavailable or returns incomplete data, the system automatically uses Docling to successfully parse the document. This can be tested by simulating failures.
	•	The code is well-structured, passes all tests (including new tests written for this feature), and is compliant with style guidelines.
	•	Documentation is updated, and developers can run the parser after configuring their Mistral API key, with clear instructions provided.

⸻

Proposed GitHub Issues (Tasks Breakdown)

To implement this feature, the work will be organized into a series of actionable GitHub issues:
	1.	Design & Architecture: Mistral Integration Plan – Prepare the technical design for integrating Mistral OCR.
	•	Description: Finalize how the Mistral API will be called and how its output will map to our data structures. Define the interface for the new doc converter module and how fallback will be orchestrated. Identify any changes needed in the codebase structure.
	•	Deliverable: Design notes (could be the PRD or a summarized version) and team agreement on approach.
	•	Acceptance Criteria: All maintainers sign off on the approach, ensuring it meets requirements.
	2.	Implement Mistral OCR Converter Module – Develop the core functionality to call Mistral API and get Markdown+images.
	•	Description: Create a new Python module or function (e.g., mistral_converter.py) that takes a PDF path and returns the parsed content. Use Mistral’s SDK (mistralai library) with model="mistral-ocr-latest". Include support for include_image_base64=True to retrieve images . Handle exceptions and invalid responses gracefully.
	•	Acceptance Criteria: Calling this function on a sample PDF returns a structured result (e.g., a dict with pages or a combined markdown and images list) without errors. The function uses an API key from config and logs errors appropriately on failure.
	3.	Remove Marker Dependencies and References – Purge the Marker tool from the project.
	•	Description: Remove any import or usage of marker in the code. Delete or disable code paths that invoked Marker. Update dependency files (requirements.txt or pyproject.toml) to drop marker. If there is any Marker-specific parsing logic that’s still needed (e.g., post-processing of marker output), refactor or remove it, ensuring that functionality is covered by Mistral or not needed.
	•	Acceptance Criteria: The project runs without Marker installed. No references to Marker exist in the code or docs. All tests relating to old marker functionality are either removed or updated to the new method.
	4.	Integrate Docling Fallback Logic – Ensure Docling is used when Mistral fails or output is incomplete.
	•	Description: Implement a function to invoke Docling (if not already present) to parse the PDF. Possibly reuse existing docling integration in the repo. Develop the logic to decide when to fallback: create utility (e.g., validate_mistral_output()) that checks for errors or missing content as per design. If validation fails, call Docling parser and get its output.
	•	Acceptance Criteria: Intentionally cause a Mistral failure (e.g., by providing an invalid API key or corrupt PDF) and verify that the code catches the error and successfully uses Docling to produce the outputs. Also simulate incomplete output (maybe stub the Mistral response to drop some pages) and verify fallback triggers. Docling output is correctly transformed to the same JSON format.
	5.	Output JSON Construction & Compatibility – Map Mistral/Docling results to the required JSON files.
	•	Description: Write the logic to construct _raw_content.json, _images.json, _tables.json from the parser output. This includes parsing markdown for sections, extracting figure captions and base64 images, and handling tables. Ensure section hierarchy in JSON matches input PDF structure. Implement any needed cleaning (like removing repeated headers/footers, if needed).
	•	Acceptance Criteria: For a given PDF, the new JSON outputs contain expected keys/structure. For example, _raw_content.json has entries for all major sections of the paper, and _images.json has an entry for each figure with correct caption and non-empty base64 data. If possible, compare with an older output to ensure compatibility (excluding minor text differences). Downstream components (Planner) can run using these files without modifications.
	6.	Support Large PDF Batching – Implement handling of PDFs exceeding 1000 pages.
	•	Description: Add a check for PDF page count or file size. If beyond Mistral’s limit (1000 pages or 50MB), split the PDF. This task includes writing a helper to split a PDF (could use PyPDF or similar library), calling Mistral on each part, and then merging the results. Also consider using Mistral’s batch mode if available.
	•	Acceptance Criteria: Able to parse a very large PDF (test with an artificially duplicated PDF to simulate >1000 pages). The code processes it in chunks and outputs a single combined set of JSON files. All parts of the document appear in the final content. No crash or API error on large input (or if there is, it’s handled with an explanatory message).
	7.	Error Handling & Logging – Robust error management for the new parser.
	•	Description: Audit the new code to ensure all failure modes are handled. For example, no API key, API rate limit errors, network issues, JSON parse errors, etc. Implement logging messages for success/failure at key points (e.g., “Mistral OCR parsing successful, X pages processed”, or “Mistral OCR failed, switching to Docling”). Ensure that an exception in parsing doesn’t crash the whole application without a clear message. Possibly integrate with any existing logging framework used by Paper2Poster.
	•	Acceptance Criteria: Simulate different error scenarios and confirm the system logs appropriate messages and continues where possible. E.g., run without setting MISTRAL_API_KEY and see that it logs an error and uses fallback or aborts gracefully. Introduce a fake network failure and see that it retries or falls back as designed. No uncaught exceptions should propagate up from the parser.
	8.	Unit & Integration Tests for Parser – Write tests to verify the new functionality.
	•	Description: Add new test cases covering Mistral integration and fallback. This includes unit tests for functions like output validation and merging logic, as well as integration tests that might call the actual API (if feasible and not too slow/costly) or use recorded responses. If direct API testing is not ideal, use mocking to simulate Mistral responses. Also include a test for the end-to-end parser on a known PDF (this could be a small PDF included in test resources) to ensure we get expected JSON results.
	•	Acceptance Criteria: All new tests pass. Code coverage for the parser module is significantly improved (aim for >90% on new code). Tests should cover both the success path (Mistral works) and the fallback path.
	9.	Documentation Updates – Revise documentation to reflect the new parser pipeline.
	•	Description: Update the README and any relevant docs. Remove mentions of Marker and add instructions for Mistral OCR (e.g., how to obtain API key, any installation needed). Document the new environment variable or config for the API key. Also, describe how the fallback works (so users/devs are aware that Docling is used if Mistral fails). If any example output is shown in docs, update it if needed. Additionally, update any usage examples or command-line interfaces that might have been tied to Marker.
	•	Acceptance Criteria: Documentation is clear and accurate. A new user following the README can set up and run the parser without needing to know about Marker. Project maintainers review the docs and confirm they are up-to-date and helpful.
	10.	Code Review and Refactoring – Ensure code quality and maintainability.
	•	Description: After implementation, perform a thorough code review (could be a peer review via Pull Request). Refactor any parts of the code that are overly complex or not in line with project style. Simplify logic where possible (for example, if some steps can be combined or if certain utility functions can be reused). Remove any leftover dead code from the transition. Ensure naming is consistent (e.g., if Marker was used in variable names, rename to more generic terms).
	•	Acceptance Criteria: Code passes review with no major issues. The project’s CI (linting, formatting checks) all pass. The final code is considered clean and maintainable by the team. Future contributors should be able to understand the parser module easily thanks to clear structure and comments.

Each issue will be addressed in order, with some potentially happening in parallel (for example, documentation can be updated as features near completion). Upon closing all these issues, we should have a fully functional Mistral-powered parser integrated into Paper2Poster, with all tests passing and no regression in functionality.
