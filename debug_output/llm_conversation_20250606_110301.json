{
  "timestamp": "20250606_110301",
  "model": "gemini-2.5-pro-preview",
  "prompt_version": 2,
  "total_tokens": 24529,
  "input_tokens": 22834,
  "output_tokens": 3396,
  "messages": [
    {
      "role": "system",
      "content": "You are the author of the paper, and you will create a poster for the paper.",
      "name": null
    },
    {
      "role": "user",
      "content": "You are a document content divider and extractor specialist, expert in dividing and extracting content from various types of documents and reorganizing it into a two-level json format for later poster generation.\n\nBased on given markdown document, generate a JSON output for later poster generation, make sure the output is concise and focused.\nStep-by-Step Instructions:\n1. Identify Sections and Subsections in document and identify sections and subsections based on the heading levels and logical structure.\n\n2. Divide Content: Reorganize the content into sections and subsections, ensuring that each subsection contains approximately 500 words.\n\n3. Refine Titles: Create titles for each section with at most 3 words.\n\n4. Remove Unwanted Elements: Eliminate any unwanted elements such as headers, footers, text surrounded by `~~` indicating deletion.\n\n5. Refine Text: For content, you should keep as much raw text as possible. Do not include citations.\n\n6. Length: you should control the length of each section, according to their importance according to your understanding of the paper. For important sections, their content should be long.\n\n7. Make sure there is a poster title section at the beginning, and it should contain information like paper title, author, organization etc.\n\n8. The \"meta\" key contains the meta information of the poster, where the title should be the raw title of the paper and is not summarized.\n\n9. Ther **must** be a section for the poster title.\n\nExample Output:\n{\n    \"meta\": {\n        \"poster_title\": \"raw title of the paper\",\n        \"authors\": \"authors of the paper\",\n        \"affiliations\": \"affiliations of the authors\"\n    },\n    \"sections\": [\n        {\n            \"title\": \"Poster Title & Author\",\n            \"content\": \"content of poster title and author\"\n        },\n        {\n            \"title\": \"title of section1\",\n            \"content\": \"content of section 1\"\n        },\n        {\n            \"title\": \"title of section2\",\n            \"content\": \"content of section 2\"\n        }\n    ]\n}\n\nGive your output in JSON format\nInput:\n# Earth Mover's Distance based Similarity Search at Scale \n\nYu Tang ${ }^{\\dagger}$, Leong Hou U ${ }^{\\ddagger}$, Yilun Cai ${ }^{\\dagger}$, Nikos Mamoulis ${ }^{\\dagger}$, Reynold Cheng ${ }^{\\dagger}$<br>${ }^{\\dagger}$ The University of Hong Kong ${ }^{\\ddagger}$ University of Macau<br>${ }^{\\dagger}$ (ytang, ylcai, nikos, ckcheng)@cs.hku.hk ${ }^{\\dagger}$ ryanlhu@umac.mo\n\n\n#### Abstract\n\nEarth Mover's Distance (EMD), as a similarity measure, has received a lot of attention in the fields of multimedia and probabilistic databases, computer vision, image retrieval, machine learning, etc. EMD on multidimensional histograms provides better distinguishability between the objects approximated by the histograms (e.g., images), compared to classic measures like Euclidean distance. Despite its usefulness, EMD has a high computational cost; therefore, a number of effective filtering methods have been proposed, to reduce the pairs of histograms for which the exact EMD has to be computed, during similarity search. Still, EMD calculations in the refinement step remain the bottleneck of the whole similarity search process. In this paper, we focus on optimizing the refinement phase of EMD-based similarity search by (i) adapting an efficient min-cost flow algorithm (SIA) for EMD computation, (ii) proposing a dynamic distance bound, which can be used to terminate an EMD refinement early, and (iii) proposing a dynamic refinement order for the candidates which, paired with a concurrent EMD refinement strategy, reduces the amount of needless computations. Our proposed techniques are orthogonal to and can be easily integrated with the state-of-the-art filtering techniques, reducing the cost of EMD-based similarity queries by orders of magnitude.\n\n\n## 1. INTRODUCTION\n\nGiven two histograms (e.g., probability distributions), their Earth Mover's Distance (EMD) is defined as the minimum amount of work to transform one histogram into the other. EMD is robust to outliers and small shifts of values among histogram bins [20], improving the quality of similarity search in different domain areas, such as computer vision [19, 21], machine learning [6, 9], information retrieval [23, 24], probabilistic [25, 32] and multimedia databases [5, 30]. Typically, the EMD between two histograms is modeled and solved as a linear optimization problem, the min-cost flow problem, which requires super-cubic time. The high computational cost of EMD restricts its applicability to datasets of lowscale. For example, in computer vision applications, the quality of results is typically compromised by the use of low-granularity histograms, to render EMD-based similarity retrieval feasible [22,30].\n\n[^0]EMD-based similarity search has been extensively studied in [5, $25,30,32]$. Given a query histogram $\\mathbf{q}$ and a database of histogram objects $\\mathcal{D}$, the objective is to find the $k$ nearest neighbors of $\\mathbf{q}$ in $\\mathcal{D}$. All these works adopt the filter-and-refinement framework; to evaluate a query, unpromising objects (or object groups) are filtered out, by utilizing various effective EMD lower bound estimations, based on centroids and projections [7], dimensionality reduction [30], primal-dual space [32], normal distributions [25], etc. Actual EMD calculations ${ }^{1}$ are applied only between $\\mathbf{q}$ and all objects that survive the filter step. Thus, the primary focus of previous research has been in tightening the lower bounds such that more objects can be pruned at the filter step. For instance, [25] demonstrate that the projection-based lower bound can be up to $90 \\%$ of the actual EMD. However, the effectiveness of a lower bound largely depends on various factors, such as the dimensionality and granularity of histograms, the data distribution, and the parameters of the similarity query (e.g., $k$ ). In particular, for large-scale datasets (e.g., 1 M cardinality and/or 1 K histogram dimensionality), the current state-of-the-art solution [25] is not feasible, due to the extreme cost of the refinement step. For instance, based on the experiments in [25], it may take 10 minutes $^{2}$ to complete one $k$-NN query on a dataset with 1 M objects even when $99 \\%$ of objects are filtered out.\n\nIn view of this, we focus on optimizing the refinement phase of EMD-based similarity search. Calculating the EMD between two object histograms is equivalent to finding the minimum-cost flow (MCF) in a bipartite network, where each vertex indicates a histogram bin and edges connect bins from different object histograms. Techniques from operations research [1], such as network simplex, primal-dual, successive shortest path, and cost-scaling can be used to solve MCF. However, these solutions do not scale well with the number of histogram bins since their computations rely on a complete bipartite network. For example, consider two histograms having 1 K bins each, and the corresponding flow graph (bipartite network) with $1 \\mathrm{M}(1 \\mathrm{~K} \\times 1 \\mathrm{~K})$ edges in total. Constructing and using this graph for solving MCF requires high computational resources. To alleviate this problem, we adapt a simplified graph incremental algorithm (SIA), originally proposed for assignmentjoins in spatial databases [29], which incrementally constructs the flow graph during the flow calculations based on the edge weights. Our adaptation significantly reduces the EMD computation time.\n\nMin-cost flow algorithms, such as SIA, only aim at efficiently evaluating a single EMD computation but they do not exploit the execution plan of EMD-based similarity queries. In other words, by integrating SIA into the filter-and-refinement framework as a black-box module, the number of EMD calculations is not affected,\n\n[^1]\n[^0]:    This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain permission prior to any use beyond those covered by the license. Contact copyright holder by emailing info@vldb.org. Articles from this volume were invited to present their results at the 40th International Conference on Very Large Data Bases, September 1st - 5th 2014, Hangzhou, China.\n    Proceedings of the VLDB Endowment, Vol. 7, No. 4\n    Copyright 2013 VLDB Endowment 2150-8097/13/12.\n\n[^1]:    ${ }^{1}$ By EMD calculation we refer to the entire run of an algorithmic process that computes the EMD between two histograms.\n    ${ }^{2} \\mathrm{~A}$ linear estimation derived from the IRMA experiment in [25].\n\nand every calculation is still conducted at its entirety. In our study, we observe that it is possible to incrementally derive and tighten a running lower bound for the EMD during the SIA calculation. Based on this, we introduce a progressive bounding (PB) technique, which can terminate the SIA calculations early for histograms that are no longer promising to the similarity query. In addition, we propose a dynamic refinement ordering (DRO) technique, which concurrently handles and dynamically re-orders multiple EMD calculations. These two techniques greatly reduce the computations at the refinement phase of EMD-based similarity search, boosting the search performance.\n\nPB and DRO can be seamlessly integrated with any existing (and future) filtering techniques. We show by experimentation that our techniques can compute EMD-based similarity queries one to two orders of magnitude faster, compared to the current state-of-theart [25]. To the best of our knowledge, ours is the first study on this subject that considers datasets of million-scale on the object cardinality and thousand-scale on the histogram dimensionality.\n\nThe rest of the paper is organized as follows. Section 2 formally defines EMD, presents a min-cost flow algorithm for its computation, and discusses the standard filter-and-refinement framework used for EMD-based similarity queries. Section 3 describes SIA, an optimized implementation of the successive shortest path MCF algorithm. Section 4 presents our progressive bounding and dynamic refinement ordering techniques. Section 5 includes an extensive experimental evaluation which demonstrates the effectiveness of our techniques. Related work is presented in Section 6. Finally, Section 7 concludes the paper with a discussion about future work.\n\n## 2. PRELIMINARIES\n\nThe Earth Mover's Distance (EMD), first introduced by the computer vision community in [23,24], is a distance function that measures the dissimilarity of two histograms (e.g., probability or feature distributions). Given two histograms $\\mathbf{q}=\\left(q_{1}, \\ldots, q_{n}\\right)$ and $\\mathbf{p}=\\left(p_{1}, \\ldots, p_{n}\\right)$, each having $n$ bins, a flow matrix $\\mathbf{F}$, where $f_{i, j}$ indicates flow (i.e., earth) to move from $q_{i}$ to $p_{j}$, and a cost matrix $\\mathbf{C}$, where $c_{i, j}$ models cost of moving flow from the $i$-th bin to the $j$-th bin, we can define the total cost of moving unit flow according to $\\mathbf{F}$ and $\\mathbf{C}$ between $\\mathbf{q}$ and $\\mathbf{p}$ as\n\n$$\nd(\\mathbf{q}, \\mathbf{p})=\\sum_{i=1}^{n} \\sum_{j=1}^{n} f_{i, j} c_{i, j}\n$$\n\nThe cost matrix (a.k.a., ground distance) $\\mathbf{C}$ can be designed by domain experts and/or derived from a mathematical formula [25, 32]. Intuitively, $c_{i, i}=0$ and the larger the distance between $i$ and $j$ in the bin space, the larger $c_{i, j}$ is. ${ }^{3}$ Assuming that $\\mathbf{q}$ and $\\mathbf{p}$ are normalized such that $\\sum_{i=1}^{n} q_{i}=\\sum_{i=1}^{n} p_{i}$, the EMD between $\\mathbf{q}$ and $\\mathbf{p}$ is formally defined as follows:\n\n$$\n\\begin{gathered}\ne m d(\\mathbf{q}, \\mathbf{p})=\\min _{\\mathbf{p}} d(\\mathbf{q}, \\mathbf{p}) \\\\\n\\text { such that } \\forall i, j \\in[1, n]: f_{i, j} \\geq 0 \\\\\n\\forall i \\in[1, n]: \\sum_{j=1}^{n} f_{i, j}=q_{i} \\\\\n\\text { and } \\forall j \\in[1, n]: \\sum_{i=1}^{n} f_{i, j}=p_{j}\n\\end{gathered}\n$$\n\n[^0]![img-0.jpeg](img-0.jpeg)\n\nFigure 1: A concrete example of online music library analysis\n\nTable 1: Cost matrix $\\mathbf{C}$ of 4 music genres\n\n|  | $p_{1}$ | $p_{2}$ | $p_{3}$ | $p_{4}$ |\n| :--: | :--: | :--: | :--: | :--: |\n| $q_{1}$ | 0 | 0.9 | 0.1 | 0.7 |\n| $q_{2}$ | 0.9 | 0 | 0.6 | 0.9 |\n| $q_{3}$ | 0.1 | 0.6 | 0 | 0.3 |\n| $q_{4}$ | 0.7 | 0.9 | 0.3 | 0 |\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: The flow network of the music example\n\n\n[^0]:    ${ }^{3}$ The motivating example of [32] partitions a 2-dimensional feature space (humidity and temperature) into $4 \\times 4$ cells based on the range of domain values. The cost $c_{i, j}$ between bins $i$ and $j$ is represented by their Euclidean distance of the corresponding cells.\n\n### 2.1 Computing the EMD\n\nEMD can be computed using linear programming [12] and network flow algorithms [1]. We now explain how EMD computation can be modeled and solved as a network flow problem. We first construct a bipartite flow network (see Figure 2(a) for the example of Figure 1), where the vertices are derived from the histogram bins (e.g., music genres) and the edges connect the bins between the two histograms. Each edge carries a cost according to the corresponding cell of the cost matrix. The flow capacity of each vertex corresponds to the value of the corresponding histogram bin. For instance, the flow capacity of vertex $q_{1}$ (i.e., $R \\& B$ of $\\mathbf{q}$ ) in Figure 2(a) is set to 3 according to Figure 1(a). Finding the minimum-cost flow in this bipartite graph is equivalent to finding the EMD from $\\mathbf{q}$ to $\\mathbf{p}$. Each vertex of $\\mathbf{q}$ should send total flow equal to its capacity and each vertex of $\\mathbf{p}$ should receive total flow equal to its capacity. The minimum-cost flow is shown in Figure 2(b). On each edge $e\\left(q_{i}, p_{j}\\right)$, the label $f_{i, j} / \\operatorname{cap}_{i, j}$ shows the flow $f_{i, j}$ sent from the origin vertex $q_{i}$ and the capacity $\\operatorname{cap}_{i, j}$ of that edge (i.e., the maximum flow which could possibly be sent from $q_{i}$ to $p_{j}$ ). The edge capacity $\\operatorname{cap}_{i, j}$ is the minimum capacity of the two end-vertices; e.g., the capacity of $e\\left(q_{2}, p_{4}\\right)$ is 3 $\\left(=\\min \\left\\{\\operatorname{cap}_{q_{2}}, \\operatorname{cap}_{p_{4}}\\right\\}=\\min \\{4,3\\}\\right)$.\n\nThe successive shortest path (SSP) algorithm [1] is the most representative algorithm in the category of the network flow based solutions. SSP iteratively computes and augments paths that (i) start from a vertex $q_{i}$ which has remaining flow capacity, (ii) terminate to a vertex $p_{i}$ which also has remaining flow capacity, and (iii) nodes from $\\mathbf{q}$ and $\\mathbf{p}$ are alternated in these paths. A valid path should include feasible edges only. Given a flow graph, an edge is feasible if there is remaining flow capacity on the edge. When augmenting a flow $f_{i, j}$ on an edge $e\\left(q_{i}, p_{j}\\right)$, we subtract $f_{i, j}$ from the capacity of $e\\left(q_{i}, p_{j}\\right)$ and add $f_{i, j}$ to the flow capacity of $e\\left(p_{j}, q_{i}\\right)$. In our running example, initially, no flow has been augmented on any edge (i.e., the $f_{i, j}$ labels of all edges are set to 0 ); thus, edge $e\\left(q_{2}, p_{4}\\right)$ (illustrated in Figure 3(a)) is feasible since the remaining flow capacity from $q_{2}$ to $p_{4}$ is 3 (illustrated by the number on the dashed line). If we augment 1 unit of flow on $e\\left(q_{2}, p_{4}\\right)$, we subtract 1 from the capacity of $e\\left(q_{2}, p_{4}\\right)$ and add 1 to the capacity of $e\\left(p_{4}, q_{2}\\right)$ (as shown in Figure 3(b)). Note that $e\\left(p_{4}, q_{2}\\right)$ is not a physical edge in $G$, as there are only directed edges from $\\mathbf{q}$ to $\\mathbf{p}$ but not viceversa. However, during path computation, SSP traverses also reverse edges provided that they are feasible. A formal definition of feasible edges is given below. The capacity of a non-physical edge (i.e., reverse edge) $e\\left(p_{j}, q_{i}\\right)$ always equals to the flow $f_{i, j}$ currently on edge $e\\left(q_{i}, p_{j}\\right)$.\n\nDefinition 1 (Feasible Edge). Given a flow graph, a physical edge $e\\left(q_{i}, p_{j}\\right)$ is feasible if $f_{i, j}<\\operatorname{cap}_{i, j}$; a non-physical (reverse) edge $e\\left(p_{j}, q_{i}\\right)$ is feasible if $f_{i, j}>0$.\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: Augmenting 1 flow on edge $e\\left(q_{s}, p_{h}\\right)$\nBesides, the cost $c_{f}(u, v)$ of an edge $e(u, v)$ is determined by its physical existence in the flow graph and the cost matrix $\\mathbf{C}$ :\n\nDefinition 2 (Cost of Feasible Edge). The cost of a physical edge $e\\left(q_{i}, p_{j}\\right)$ is $c_{f}\\left(q_{i}, p_{j}\\right)=c_{i, j}$, while the cost of a non-physical (reverse) edge $e\\left(p_{j}, q_{i}\\right)$ is $c_{f}\\left(p_{j}, q_{i}\\right)=-c_{i, j}$.\n\nFor instance, the reverse edge $e\\left(p_{4}, q_{2}\\right)$ in Figure 3(b) is feasible. Its cost is $c_{f}\\left(p_{4}, q_{2}\\right)=-c_{2,4}=-0.9$ since $e\\left(p_{4}, q_{2}\\right)$ is not a physical edge. To calculate the min-cost flow (i.e., EMD), SSP iteratively searches for the feasible path having the minimum cost. As discussed above, a feasible path starts from a vertex in $\\mathbf{q}$, which still has positive flow capacity, ends at a vertex in $\\mathbf{p}$ with positive flow capacity and includes only feasible edges. For instance, there is a feasible path highlighted by three dashed lines in Figure 2(c); the path starts at $q_{1}$, passes $p_{3}$ and $q_{2}$, and finally reaches $p_{4}$. The cost of this path is $c_{f}\\left(q_{1}, p_{3}\\right)+c_{f}\\left(p_{3}, q_{2}\\right)+c_{f}\\left(q_{2}, p_{4}\\right)=$ $0.1+(-0.6)+0.9=0.4$. SSP selects the feasible path with the lowest cost and augments the maximum possible flow along the path. The augmented flow is determined by the minimum of the following quantities: (i) the remaining flow capacity at the source node, (ii) the remaining flow capacity at the destination, (iii) the minimum remaining capacity of all edges on the path. For example, Figure 2(b) shows the result of augmenting the path shown by the three dashed lines in Figure 2(c). The augmentation adds 1 flow unit to all physical edges on the path (i.e., $e\\left(q_{1}, p_{3}\\right)$ and $e\\left(q_{2}, p_{4}\\right)$ ), subtracts 1 flow unit from the edges, for which the reverse edge is on the path (i.e., $e\\left(q_{2}, p_{3}\\right)$ ), and updates the capacities of path edges, $q_{1}$, and $p_{4}$.\n\nComputing the EMD, using SSP requires $O(F|E| \\log |V|)$ time, where $F$ is the total number of flows we need to augment and $O(|E| \\log |V|)$ is the cost of a shortest path search on a bipartite graph with $|V|$ vertices and $|E|$ edges. After each path augmentation, the changes in the graph render the subsequent shortest path search independent from the previous one, therefore, a large number of shortest path computations should be applied. As we shall see in Section 3, we can greatly reduce the cost of SSP by a method which builds and searches the flow graph incrementally.\n\n### 2.2 Filter-and-Refinement Framework\n\n![img-3.jpeg](img-3.jpeg)\n\nFigure 4: Filter-and-refinement framework\n\nGiven a collection $\\mathcal{D}$ of histograms and a query histogram $\\mathbf{q}$, a $k$-nearest neighbor ( $k$-NN) query, finds a subset $\\mathcal{S}$ of $\\mathcal{D}$ containing $k$ histograms, such that $\\forall \\mathbf{p} \\in \\mathcal{S}, \\forall \\mathbf{p}^{\\prime} \\in \\mathcal{D} \\backslash \\mathcal{S}, \\operatorname{emd}(\\mathbf{q}, \\mathbf{p}) \\leq$ $\\operatorname{emd}\\left(\\mathbf{q}, \\mathbf{p}^{\\prime}\\right)$. The $k$-NN query is the most popular similarity search type, as the number of results is controlled by $k$ and there is no requirement for setting a similarity threshold prior to search. In previous studies $[5,7,25,30], k$-NN queries are evaluated based on a filter-and-refinement framework. The EMD $\\operatorname{emd}(\\mathbf{q}, \\mathbf{p})$ between the query histogram $\\mathbf{q}$ and every histogram $\\mathbf{p} \\in \\mathcal{D}$ is estimated with the help of lower bound filtering techniques, such as the normal distribution index [25], the full projection lower bound [7], the reduced dimension lower bound [30], and independent minimization [5]. In general, these filters are applied in an order starting from quick-and-dirty ones to slow-and-accurate ones, as shown in Figure 4. For histograms $\\mathbf{p}$ that cannot be pruned by the filters, the actual $\\operatorname{emd}(\\mathbf{q}, \\mathbf{p})$ is calculated by a black-box computation module, such as SSP or transportation simplex [12].\n\n```\nAlgorithm 1 Filter-And-Refinement for \\(k\\)-NN\n    \\(H:\\) heap, \\(\\theta\\) : pruning threshold\n    Algorithm \\(k\\)-NN(Query q, Index \\(I\\), Filters \\(\\Delta\\) )\n\\(1: \\theta:=\\infty ; H:=\\emptyset\\)\n2: while \\(I \\operatorname{getnext}(\\mathbf{q}, \\theta,\\left\\langle\\mathbf{p}, l b_{\\mathbf{p}}\\right\\rangle\\) ) do\n    for \\(\\delta_{i} \\leqslant \\Delta\\) do\n        \\(l b_{\\mathbf{p}}:=\\max \\left\\{l b_{\\mathbf{p}}, \\delta_{i}(\\mathbf{q}, \\mathbf{p})\\right\\}\\)\n        if \\(l b_{\\mathbf{p}} \\geq \\theta\\) then break loop\n    if \\(l b_{\\mathbf{p}}<\\theta\\) then\n        if \\(e m d(\\mathbf{q}, \\mathbf{p})<\\theta\\) then\n        update \\(H\\) to include \\(\\left\\langle\\mathbf{p}, e m d(\\mathbf{q}, \\mathbf{p})\\right\\rangle\\)\n        \\(\\theta:=k\\)-th EMD value in \\(H\\)\n    return \\(H\\)\n```\n\nAlgorithm 1 is a pseudocode of the filter-and-refinement framework used in previous work. Since histograms correspond to objects (e.g., images), we will use the terms objects and histograms interchangeably. First, the framework accesses objects from an index $I$, such as the normal distribution index [25] or the TBI index [32]. These indexes provide a getnext function which returns at each call an unseen object $\\mathbf{p}$ having lower bound $l b_{\\mathbf{p}}$ of $\\operatorname{emd}(\\mathbf{q}, \\mathbf{p})$ smaller than a given threshold $\\theta$ (line 2). In $k$-NN search, $\\theta$ is the $k$-th largest EMD computed so far (i.e., the distance of the current $k$-th NN of $\\mathbf{q}$ in $\\mathcal{D}$ ). At each iteration (lines 2-9), the framework accesses a histogram $\\mathbf{p} \\in \\mathcal{D}$ using the getnext function and attempts to tighten its lower bound $l b_{\\mathbf{p}}$ by applying a set $\\Delta$ of progressively more expensive and accurate lower bound estimation techniques (see Figure 4). If any of the computed lower bounds is not smaller than the pruning threshold $\\theta$ (line 5), then $\\mathbf{p}$ is filtered, i.e., the exact $\\operatorname{emd}(\\mathbf{q}, \\mathbf{p})$ needs not to be computed. Otherwise, $\\operatorname{emd}(\\mathbf{q}, \\mathbf{p})$ is essentially computed by a black-box algorithm (line 7). During search, Algorithm 1 maintains a heap $H$ with the $k$ histograms having the lowest EMD so far and the pruning threshold $\\theta$ (lines 8-9). The $k$-NN candidates are confirmed as the result, when no more objects are returned by the getnext function (line 2), i.e., all unseen objects do not satisfy the distance threshold $\\theta$.\n\n## 3. SCALING UP SSP\n\nComputing EMD by SSP requires having the complete bipartite graph between $\\mathbf{q}$ and $\\mathbf{p}$, which is quadratic to the number of histogram bins. In our previous work, we proposed an optimized version of SSP, simplified graph incremental algorithm (SIA) [29], to scale up the computation of spatial matching problems. We now show how SIA can be adopted to scale up the computation of EMD, for histograms with a large number of bins. Different from SSP, SIA incrementally constructs a partial flow graph $G^{\\prime}$ by inserting edges from the complete bipartite graph $G$ to $G^{\\prime}$. The incremental graph construction significantly reduces the search cost since the size of the partial graph $G^{\\prime}$ is typically much smaller than the complete graph $G$. We use the running example (cf. Figure 2) to demonstrate the superiority of SIA in Figure 5. Recall that the min-cost feasible path starting at $q_{1}$ (cf. Figure 2(c)) is $q_{1} \\rightarrow p_{3} \\rightarrow q_{2} \\rightarrow p_{4}$ in the complete graph $G$. Suppose a partial graph $G^{\\prime}$ is constructed based on the seen values in the partial cost matrix (Figure 5(b)), the same min-cost feasible path can be found in $G^{\\prime}$ as well. Thereby, augmenting this path in $G^{\\prime}$ is equivalent to the augmentation in $G$ which returns the same result in Figure 2(b).\n\nThe question now turns to how we can guarantee that the mincost feasible paths in $G$ and $G^{\\prime}$ are equivalent. This is done by a distance bound checking where the bound $\\Pi$ is derived from the edges not yet inserted into $G^{\\prime}$. As an intuition, if the edges in $G^{\\prime}$ are inserted incrementally in ascending order to their costs at every vertex, it is possible that the min-cost path in $G^{\\prime}$ is cheaper than all\n![img-4.jpeg](img-4.jpeg)\n(a) Partial graph, $G^{\\prime}$\n(b) The cost matrix of $G^{\\prime}$\n\nFigure 5: SIA on a partial flow graph\nfeasible paths which include edges not in $G^{\\prime}$. In this case, the path is augmented; otherwise SIA expands $G^{\\prime}$ by inserting more edges until the best feasible path in $G^{\\prime}$ has lower cost than the threshold $\\Pi^{4}$; the expansion of $G^{\\prime}$ can only reduce the cost of the feasible flow and increase $\\Pi$, as edges are inserted to $G^{\\prime}$ in increasing order of their costs.\n\n```\nAlgorithm 2 SIA BASED EMD CALCULATION\n    \\(\\Pi\\) : distance bound, \\(G^{\\prime}\\) : running subgraph\n    Algorithm \\(\\operatorname{emd}_{S I A}\\) (Histograms q, p)\n    \\(\\Pi:=0 ; G^{\\prime}:=\\emptyset\\)\n    while 3 feasible \\(q\\), do\n        \\(s p:=\\operatorname{Dijkstra}\\left(q_{i}, G^{\\prime}\\right)\\)\n        while \\(s p . \\operatorname{cost}>\\Pi\\) or \\(s p\\) doesn't reach any feasible \\(p_{j}\\) do\n            insert min-cost edge \\(e\\left(q_{i}, p_{m}\\right) \\leqslant G-G^{\\prime}\\) into \\(G^{\\prime}\\)\n            update distance bound \\(\\Pi\\)\n            \\(s p:=\\operatorname{Dijkstra}\\left(q_{i}, G^{\\prime}\\right)\\)\n        augment \\(s p\\)\n    return total augmenting cost\n```\n\nAlgorithm 2 is a pseudocode of SIA for EMD calculations. At each iteration, SIA searches the min-cost feasible path $s p$ using Dijkstra's shortest path algorithm [1] in $G^{\\prime}$ from any feasible vertex (line 3), i.e., a vertex of $\\mathbf{q}$ with remaining capacity. ${ }^{5}$ If the cost of $s p$ does not exceed the distance bound $\\Pi$ (line 4), then it must be a valid min-cost feasible path in the entire graph $G$. We augment the flow of $s p$ if $s p$ is valid (line 8). Otherwise, $G^{\\prime}$ is essentially expanded by adding more edges from $G$ (line 5) and the distance bound $\\Pi$ is updated accordingly (line 6).\n\nWe demonstrate the functionality of SIA by the example of Figure 6. Suppose that $G^{\\prime}$ contains only 6 edges and there are 9 flow capacities sent already. According to the flow capacities, $q_{1}$ is the only feasible node in $\\mathbf{q}$ but there is no feasible path currently from $q_{1}$ to any node of $\\mathbf{p}$ (see Figure 6(a)). Subsequently, we insert a new edge, $e\\left(q_{1}, p_{3}\\right)$, into $G^{\\prime}$ and now there is a shortest path, $s p=\\left\\langle e\\left(q_{1}, p_{3}\\right), e\\left(p_{3}, q_{2}\\right), e\\left(q_{2}, p_{4}\\right)\\right\\rangle$. The cost of $s p$ is 0.4 , which is smaller than the distance bound $\\Pi$, thus we return $s p$ as the result of the current search and augment 1 unit of flow from $q_{1}$ and $q_{2}$ to $p_{3}$ and $p_{4}$, while 1 unit of flow from $q_{2}$ to $p_{3}$ is canceled.\n${ }^{4}$ For clarity, $\\Pi=c_{\\max }\\left(E-E^{\\prime}\\right)-\\tau_{\\max }$, where $c_{\\max }(\\cdot)$ returns the maximum cost in a set of edges, $E^{\\prime}$ denotes the edges in $G^{\\prime}$, and $\\tau_{\\max }$ indicates the largest potential value of the vertices. As a note, $\\Pi$ can be further tightened if considering only a subset of $E^{\\prime}$ where the edges are from the vertices being visited by the current Dijkstra search. The correctness proof and optimization details are given in [29].\n${ }^{5}$ Since $G$ (and $G^{\\prime}$ ) may contain edges of negative costs (i.e., the reverse of edges that currently carries flow), Dijkstra's algorithm cannot be directly applied. To make its application possible, we need to iteratively maintain a potential value at every vertex, which transforms the costs of the feasible edges to non-negative values. The details (see [29]) are omitted for the sake of readability.\n\n![img-5.jpeg](img-5.jpeg)\n\nFigure 6: A running example of SIA-EMD\n\nIn this work, we use SIA as a module for computing the EMD between two histograms $\\mathbf{p}$ and $\\mathbf{q}$ in the refinement step of similarity search, due to its efficiency and scalability to the number of histogram bins. Still, SIA only optimizes the performance of an individual EMD calculation, whereas our ultimate objective is to minimize the overall cost of a similarity query, which may involve a large number of EMD calculations. The next section shows how we can optimize the overall cost of queries by exploiting information during the course of a SIA calculation.\n\n## 4. BOOSTING THE REFINEMENT PHASE\n\nIn this section, we propose two novel techniques, progressive bounding (PB) and dynamic refinement ordering (DRO), which boost the performance of the refinement phase during EMD-based similarity search. PB is inspired by the running time pattern of a single EMD calculation and DRO is inspired by the execution order in the filter-and-refinement framework. These two techniques make the EMD calculations at the refinement phase being handled in multiple stages and progressively instead of as one-off processes (i.e., by black-box modules).\n\n### 4.1 Analysis of EMD Calculation\n\nAccording to previous experimental studies (e.g., [25,32]), more than $95 \\%$ of the histograms on average can be filtered by the lower bound estimations; however, such high filter effectiveness is not guaranteed. To make things worse, if we process EMD-based similarity queries on large datasets (e.g., 1 M objects) having highgranularity histograms (e.g., several hundreds of bins), the refinement phase even at a very high filtering ratio (e.g., $99 \\%$ ) easily becomes the bottleneck, due to the high cost of exact EMD calculations. Although there has been ample work on improving the performance and effectiveness of the filter phase in EMD-based similarity search, to the best of our knowledge, there has been no work focusing on optimizing the refinement phase, for which off-the-shelf EMD computation techniques (such as transportation simplex or SSP) are simply applied as black-box modules.\n![img-6.jpeg](img-6.jpeg)\n\nFigure 7: SIA performance on different iterations\n\nAs discussed in Section 3, we adopt SIA for exact EMD calculations in the refinement phase. Although SIA offers great performance improvements over typical EMD solutions (e.g., SSP [1] and transportation simplex [12]), the running time during its execution increases quickly as the algorithm progresses. Figure 7(a) shows the cumulative runtime over SIA iterations during a typical EMD calculation for two histograms with 1024 bins each. This example shows that over $90 \\%$ of SIA's execution time is used in the last $20 \\%$ of its iterations. The reason behind this is that as SIA progresses, the partial graph $G^{\\prime}$ and the number of feasible edges grow and shortest path searches become much more expensive. This analysis shows that there is room for greatly improving the refinement phase of EMD-based similarity search, if SIA can be terminated before reaching its late iterations.\n\n### 4.2 Progressive Bounding\n\nIn order to terminate SIA as early as possible for objects that do not make it to the query result set, we propose a technique which progressively maintains a running lower bound emd $^{-}$and tightens $e m d^{-}$throughout the entire EMD calculation process. SIA, for a specific refinement, can terminate early if the running lower bound becomes not smaller than the current pruning threshold $\\theta$ (i.e., the $k$-th lowest EMD found so far).\n\nFirst, we use a property of SSP (and SIA); at each iteration, the minimum-cost feasible path is nonnegative when the cost matrix $\\mathbf{C}$ does not include any negative values [1]. In other words, the accumulated cost by augmenting flows at each iteration is monotonically non-decreasing, and can be used as the running lower bound of EMD. For example, Figure 7(b) illustrates the accumulation of the EMD during SIA, for two 1024-bin histograms $\\mathbf{q}$ and $\\mathbf{p}$. At each iteration, the current cost represents the accumulation of the augmented costs up to that iteration; by comparing it with the current pruning threshold $\\theta$ (ignore the other series for the moment), we can observe that SIA may terminate as soon as the current cost becomes at least equal to $\\theta$ (i.e., at around $88.3 \\%$ of the total iterations). This point corresponds to $23.4 \\%$ of SIA's total execution time (cf. Figure 7(a)). Note that the current cost remains 0 until the 1024th iteration because the shortest paths up to this iteration correspond to flows between the same bins of $\\mathbf{q}$ and $\\mathbf{p}$ (i.e., from $q_{i}$ to $p_{i}$ for some $i \\in[1, n], n=1024$ ) with zero cost each.\n![img-7.jpeg](img-7.jpeg)\n\nFigure 8: Running lower bound, $e m d^{-}$\n\nAlthough the current cost could be intuitively used as a lower bound for the final (actual) EMD, it is not sufficiently tight since it does not take the remaining feasible flow into consideration. Thereby, we propose a running lower bound, $e m d^{-}$, which not only considers the so-far accumulated flow cost, but also provides an estimation to the cost of flows yet to be augmented. As Figure 8 illustrates, $e m d^{-}$consists of the current cost (partial EMD already computed) plus a lower bound for the cost of the remaining flow. The effectiveness of $e m d^{-}$is shown in the running example of Figure 7(b), where $e m d^{-}$grows much faster compared to the\n\ncurrent cost and leads to an early termination of SIA for EMD calculations. In this example, the running lower bound $e m d^{-}$reaches the pruning threshold $\\theta$ after augmenting $61 \\%$ iterations which take only $2.84 \\%$ of the total execution time. This result demonstrates the effectiveness of $e m d^{-}$in boosting EMD-based similarity search.\n\nWe now formally define $e m d^{-}$. First, Lemma 1 shows that the costs of the minimum-cost feasible paths being augmented from a vertex are monotonically non-decreasing, which forms a basis for the definition of the bound.\n\nLemma 1 (Cost Monotonicity of Feasible Paths). Given a flow graph for which there is a sequence of $m_{i}$ minimum-cost feasible paths $s p_{q_{1}, 1}, \\ldots, s p_{q_{i}, m_{i}}$ from a node $q_{i} \\in \\mathbf{q}$ already been augmented in this order, the cost of these paths from $q_{i}$ is monotonically non-decreasing, i.e., $c_{f}\\left(s p_{q_{i}, j}\\right) \\leq c_{f}\\left(s p_{q_{i}, k}\\right), \\forall 1 \\leq j<k \\leq m_{i}$.\n\nProof. To prove the statement, it suffices to show that the cost of the shortest path from any $q_{i} \\in \\mathbf{q}$ to any $p_{j} \\in \\mathbf{p}$ is monotonically non-decreasing throughout the entire flow augmentation process; i.e., $c_{f}^{t-1}\\left(q_{i} \\rightsquigarrow p_{j}\\right) \\leq c_{f}^{t}\\left(q_{i} \\rightsquigarrow p_{j}\\right)$, where $c_{f}^{t}$ is the cost of the shortest path from $q_{i}$ to $p_{j}$ after $t$ augmentations. If this holds, then the cost of the minimum-cost feasible paths from $q_{i}$ must be monotonically non-decreasing, because at each iteration, SIA picks for $q_{i}$ the path with the minimum cost, and there is always a feasible path between any pair of nodes with remaining capacity.\n\nWe prove this by contradiction. Assume that the monotonicity of the shortest path cost from $q_{i}$ to a vertex $p_{j}$ does not hold, i.e., $c_{f}^{t-1}\\left(q_{i} \\rightsquigarrow p_{j}\\right)>c_{f}^{t}\\left(q_{i} \\rightsquigarrow p_{j}\\right)$, and assume that no path violates the monotonicity property until the $t$-th iteration. In addition, among all paths that violate the monotonicity property at the $t$-th iteration, $c_{f}^{t}\\left(q_{i} \\rightsquigarrow p_{j}\\right)$ has the minimum cost. Suppose that $q_{k}$ is the preceding vertex of $p_{j}$ in the shortest path, we have\n\n$$\nc_{f}^{t}\\left(q_{i} \\rightsquigarrow p_{j}\\right)=c_{f}^{t}\\left(q_{i} \\rightsquigarrow q_{k}\\right)+c_{f}^{t}\\left(q_{k}, p_{j}\\right) \\geq c_{f}^{t}\\left(q_{i} \\rightsquigarrow q_{k}\\right)\n$$\n\nIn addition, based on our assumption, no vertex violates the monotonicity property before the $t$-th augmentation. Thereby,\n\n$$\nc_{f}^{t-1}\\left(q_{i} \\rightsquigarrow q_{k}\\right) \\leq c_{f}^{t}\\left(q_{i} \\rightsquigarrow q_{k}\\right)\n$$\n\nBy combining the equations, we get\n\n$$\nc_{f}^{t-1}\\left(q_{i} \\rightsquigarrow p_{j}\\right)>c_{f}^{t}\\left(q_{i} \\rightsquigarrow p_{j}\\right) \\geq c_{f}^{t}\\left(q_{i} \\rightsquigarrow q_{k}\\right) \\geq c_{f}^{t-1}\\left(q_{i} \\rightsquigarrow q_{k}\\right)\n$$\n\nIf $e\\left(q_{k}, p_{j}\\right)$ is feasible in $G^{t-1}$, then:\n\n$$\n\\begin{aligned}\nc_{f}^{t-1}\\left(q_{i} \\rightsquigarrow p_{j}\\right) & \\leq c_{f}^{t-1}\\left(q_{i} \\rightsquigarrow q_{k}\\right)+c_{f}^{t-1}\\left(q_{k}, p_{j}\\right) \\\\\n& \\leq c_{f}^{t}\\left(q_{i} \\rightsquigarrow q_{k}\\right)+c_{f}^{t}\\left(q_{k}, p_{j}\\right)=c_{f}^{t}\\left(q_{i} \\rightsquigarrow p_{j}\\right)\n\\end{aligned}\n$$\n\notherwise ( $e\\left(p_{j}, q_{k}\\right)$ is feasible in $G^{t-1}$ ):\n\n$$\n\\begin{aligned}\nc_{f}^{t-1}\\left(q_{i} \\rightsquigarrow q_{k}\\right) & \\geq c_{f}^{t-1}\\left(q_{i} \\rightsquigarrow p_{j}\\right)+c_{f}^{t-1}\\left(q_{k}, p_{j}\\right) \\\\\n& >c_{f}^{t}\\left(q_{i} \\rightsquigarrow p_{j}\\right)+c_{f}^{t}\\left(q_{k}, p_{j}\\right) \\geq c_{f}^{t}\\left(q_{i} \\rightsquigarrow q_{k}\\right)\n\\end{aligned}\n$$\n\nBoth cases contradict our assumptions.\nDefinition 3 (Running Lower Bound, emd ${ }^{-}$).\nConsider a flow graph, such that for each node $q_{i} \\in \\mathbf{q}$, there is a set of $m_{i}$ augmented paths $s p_{q_{1}, 1}, \\ldots, s p_{q_{i}, m_{i}}$. The running lower bound $e m d^{-}$is defined as\n\n$$\n\\begin{aligned}\ne m d^{-}= & \\sum_{q_{i} \\in \\mathbf{q}} \\sum_{j=1}^{m_{i}} c_{f}\\left(s p_{q_{i}, j}\\right) \\cdot f\\left(s p_{q_{i}, j}\\right)+ \\\\\n& \\sum_{q_{i} \\in \\mathbf{q}} c_{f}\\left(s p_{q_{i}, m_{i}}\\right) \\cdot\\left(c a p_{q_{i}}-f_{q_{i}}\\right)\n\\end{aligned}\n$$\n\n(estimated cost)\nwhere cap ${ }_{v}$ indicates the total flow capacity of node $v$, and $f_{v}$ indicates the flow units already augmented from $v$ so far.\n\nLemma 2 (CorReCTNESS of $e m d^{-}$). The running lower bound $e m d^{-}$is monotonically non-decreasing and always not greater than $\\operatorname{emd}(\\mathbf{q}, \\mathbf{p})$ throughout the EMD calculation.\n\nProof. Trivial, due to the monotonicity of the shortest path costs from any vertex $q_{i}$ (Lemma 1) and due to the fact that all remaining flow $c a p_{q_{i}}-f_{q_{i}}$ at $q_{i}$ should be augmented in paths originating at $q_{i}$.\n\nTable 2: Four augmented paths and their costs\n\n| Iteration | feasible path | $c_{f}\\left(s p_{j}\\right)$ | $f\\left(s p_{j}\\right)$ | current cost |\n| :--: | :--: | :--: | :--: | :--: |\n| 1 | $s p_{1}=\\left\\langle e\\left(q_{1}, p_{1}\\right)\\right\\rangle$ | 0.0 | 2 | 0.0 |\n| 2 | $s p_{2}=\\left\\langle e\\left(q_{3}, p_{3}\\right)\\right\\rangle$ | 0.0 | 2 | 0.0 |\n| 3 | $s p_{3}=\\left\\langle e\\left(q_{2}, p_{2}\\right)\\right\\rangle$ | 0.0 | 1 | 0.0 |\n| 4 | $s p_{4}=\\left\\langle e\\left(q_{2}, p_{3}\\right)\\right\\rangle$ | 0.6 | 2 | 1.2 |\n\nUsing Definition 3, we can compute $e m d^{-}$by adding to current cost an estimated lower bound for all remaining feasible paths originating at each vertex $q_{i}$, based on the node's remaining capacity (i.e., $c a p_{q_{i}}-f_{q_{i}}$ ) and the cost of the last augmented feasible path from $q_{i}$. Lemma 2 proves the correctness of the bound. The example of Figure 8 illustrates the computation of $e m d^{-}$. There are 4 paths already augmented; their costs and iteration order are shown in Table 2. The current cost is $1.2\\left(=c_{f}\\left(s p_{4}\\right) f\\left(s p_{4}\\right)=0.6 \\quad 2\\right)$ and the estimated cost of the remaining flows (computed using the nodes of $\\mathbf{q}$ in the dashed-line region) is $0.6\\left(=c_{f}\\left(s p_{4}\\right)\\left(c a p_{q_{2}}\\right.\\right.$ $-$ $\\left.f_{q_{2}}\\right)=0.6 \\cdot(4-3)$ ). Thereby, $e m d^{-}$is 1.8 which is much tighter compared to just using the current cost as the lower bound.\n\nNote that the time of updating $e m d^{-}$during SIA is negligible. Whenever a new feasible path $s p$ from a vertex $q_{i}$ is augmented (i.e., at each iteration of SIA), we refine $e m d^{-}$by: (i) adding the augmentation cost of $s p\\left(c_{f}(s p) \\cdot f(s p)\\right)$ to the current cost component of $e m d^{-}$; (ii) subtracting the previous estimated cost for $q_{i}$ in the estimated cost component; (iii) adding the new estimated cost of $q_{i}$. Each of these three increments takes constant time (in fact, (ii) is already cached), so the update of $e m d^{-}$takes $O(1)$ time.\n\nSumming up, our progressive bounding ( PB ) approach, during the EMD calculation for a candidate, progressively maintains and tightens the running lower bound $e m d^{-}$and prunes the object as soon as the intermediate $e m d^{-}$reaches the pruning threshold $\\theta$. PB saves unnecessary computations at the latter (expensive) stages of SIA for candidate objects that do not make it to the $k$-NN result (i.e., conducting only the necessary portion of the entire flow calculation), and thus reduces the cost of each individual refinement.\n\n### 4.3 Sensitivity to Refinement Order\n\nSo far, we have discussed how to terminate a single EMD calculation early by the progressive bounding technique. Still, the performance of a $k$-NN query does not depend only on individual EMD calculations but also on the amount and progresses of EMD calculations. According to Algorithm 1, the EMD for an object $\\mathbf{p}$ is essentially refined if all estimated lower bounds $l b_{\\mathbf{p}}$ (at the filter phase) are smaller than the pruning threshold $\\theta$ (line 6 of Algorithm 1), where $\\theta$ is the $k$-th best-so-far EMD value.\n\nFigure 9 shows the EMD values of the accessed objects during a 4-NN query execution using Algorithm 1. For each candidate $\\mathbf{p}$, a bar shows the real EMD value $e m d(\\mathbf{q}, \\mathbf{p})$, the best lower bound $l b_{\\mathbf{p}}$ from the filter phase, and the pruning threshold $\\theta$ at the time of $\\mathbf{p}$ 's verification. Every refined candidate can potentially decrease the pruning threshold $\\theta$ if it replaces another object in the current $k$-NN set $\\mathcal{H}$. For instance, $\\theta$ is decreased after accessing and refining the 6 -th object. Observe that the order by which the objects are accessed is not consistent with their real EMD values. On the other\n\n![img-8.jpeg](img-8.jpeg)\n\nFigure 9: Access order of a $k$-NN search\nhand, the amount of EMD calculations would have been greatly reduced if we had considered the objects in a better order. For instance, if we had accessed the 6 -th and 7 -th objects before the 5 -th object $\\mathbf{p}_{5}$, then $\\mathbf{p}_{5}$ would have been filtered since its best lower bound would have been larger than $\\theta$ in this case. Note that a better access order not only filters more objects, but also decreases the pruning threshold $\\theta$ faster such that individual EMD calculations can be terminated earlier owing to the progressing bounding technique. Unfortunately, the access order of the state-of-the-art filter-and-refinement framework is based on the getnext function (provided by the normal distribution index [25] or the TBI index [32]), which just returns any unseen object $\\mathbf{p}$ having lower bound smaller than a given threshold $\\theta$. As shown in this example, there is room to improve the access order such that objects that are likely to have smaller EMD values have higher chances to be refined earlier. Motivated by this analysis, we propose a novel technique that defines and follows a dynamic access order of the candidates.\n\n### 4.4 Dynamic Refinement Ordering\n\nThe main idea behind our dynamic refinement ordering (DRO) approach in refining candidates during EMD-based similarity search is to conduct the refinement for multiple candidates concurrently. Thus, given a priority layer $P L$ set of $b$ candidates $(P L \\subseteq \\mathcal{D})$, such that each $\\mathbf{p} \\in P L$ passes all filters based on the current threshold $\\theta$, the objective of DRO is to refine all objects in $P L$ concurrently, by augmenting paths to the EMD of $\\mathbf{p} \\in P L$, which is currently the most promising object in $P L$. Intuitively, an object is promising to augment flows on its corresponding EMD, if the augmentations can make the threshold $\\theta$ lower or prune objects from $P L$. For instance, the running lower bound $e m d^{-}$can be used to prioritize objects, since augmenting an object having the lowest $e m d^{-}$may update the best $k$ objects found so far and decrease the threshold $\\theta$ early. The augmentations result in the increase (i.e., tightening) of $e m d_{\\mathbf{p}}^{-k}$, which may cause another object $\\mathbf{p}^{\\prime}$ to take the place of $\\mathbf{p}$ as the most promising one and be refined by DRO in the next step. Thus, DRO always refines the object $\\mathbf{p}$ with the currently best promising value and checks whether $\\mathbf{p}$ can be pruned after updating (i.e., increasing) $e m d_{\\mathbf{p}}^{-}$. If this pruning happens, $\\mathbf{p}$ is replaced by another candidate object in $P L$. DRO also keeps track of the upper bound $e m d_{\\mathbf{p}}^{+}$for each object $\\mathbf{p} \\in P L$; if the currently refined object's upper bound becomes smaller than $\\theta$, then the current top- $k$ result is updated to include $\\mathbf{p}$. The details on how to compute and update an upper bound of a partially computed EMD are given in Section 4.5. DRO continues until $P L$ becomes empty.\n\nAlgorithm 3 is a pseudocode of the DRO strategy for EMDbased $k$-NN search. DRO uses function getnext_filter to get the next object from $\\mathcal{D}$, which passes all filters with respect to the current threshold $\\theta$. First, DRO calls this function $b$ times to form the initial $P L . P L$ is stored as a priority queue, in which the top\n\n[^0]Algorithm 3 Dynamic Refinement Ordering\n$H, P L$ : heap, $\\theta$ : pruning threshold\nFunction getnext_filter(Query q, Index $I$, Filters $\\Delta$ )\nwhile $I$. getnext $\\left(\\mathbf{q}, \\theta,\\left\\langle\\mathbf{p}, l b_{\\mathbf{p}}\\right\\rangle\\right)$ do\nfor $\\delta_{i} \\in \\Delta$ do\n$b_{\\mathbf{p}}:=\\max \\left\\{l b_{\\mathbf{p}}, \\delta_{i}(\\mathbf{q}, \\mathbf{p})\\right\\}$\nif $l b_{\\mathbf{p}} \\geq \\theta$ then break loop\nif $l b_{\\mathbf{p}}<\\theta$ then return $\\left\\langle\\mathbf{p}, l b_{\\mathbf{p}}, \\infty\\right\\rangle$\nAlgorithm DRO-kNN(Query q, Index $I$, size $b$ )\n$6: \\theta:=\\infty ; H:=\\emptyset ; P L:=\\emptyset$\nwhile $|P L|<b$ do\n$P L:=P L \\cup$ getnext_filter $(\\mathbf{q}, I, \\Delta)$\nwhile $|P L| \\neq \\emptyset$ do\npop $\\left\\langle\\mathbf{p}, e m d_{\\mathbf{p}}^{+}, e m d_{\\mathbf{p}}^{+}\\right\\rangle$ from $P L \\quad \\triangleright$ e.g., lowest $e m d_{\\mathbf{p}}^{+}$\nwhile $e m d_{\\mathbf{p}}^{+}<P L . t o p() . e m d^{-}$do\naugment next shortest path in $e m d(\\mathbf{q}, \\mathbf{p})$\nif $e m d_{\\mathbf{p}}^{+} \\geq \\theta \\vee e m d_{\\mathbf{p}}^{+}=e m d_{\\mathbf{p}}^{+}$then\n$P L:=P L \\cup$ getnext_filter $(\\mathbf{q}, I, \\Delta)$\nbreak loop\nif $e m d_{\\mathbf{p}}^{+}<\\theta$ then\nupdate $H$ to include the new $\\left\\langle\\mathbf{p}, e m d_{\\mathbf{p}}^{+}\\right\\rangle$\n$\\theta:=k$-th EMD value in $H$\nfor all $\\mathbf{p} \\in P L$ such that $e m d_{\\mathbf{p}}^{+} \\geq \\theta$ do\nremove $\\mathbf{p}$ from $P L$\n$P L:=P L \\cup$ getnext_filter $(\\mathbf{q}, I, \\Delta)$\nif loop not broken then\n$P L:=P L \\cup \\mathbf{p} \\quad \\triangleright$ add $\\mathbf{p}$ back to $P L$\nreturn $H$\nelement is the object with the smallest $e m d^{-}$. At each iteration, DRO de-heaps the top object $\\mathbf{p}$ in $P L$ and progressively refines the current $e m d(\\mathbf{q}, \\mathbf{p})$ by iteratively augmenting shortest paths (i.e., using SIA) while $e m d_{\\mathbf{p}}^{+}$is still smaller than $P L . t o p() . e m d^{-}$(i.e, the smallest $e m d^{-}$in $P L$ ). A path augmentation increases $e m d_{\\mathbf{p}}^{+}$ and decreases $e m d_{\\mathbf{p}}^{+}$. If $e m d_{\\mathbf{p}}^{+} \\geq \\theta$, then the current object $\\mathbf{p}$ is pruned because in the best case it cannot become better than the current $k$ nearest neighbors (see Section 4.2); DRO calls function getnext_filter to add another candidate in $P L$ in place of $\\mathbf{p}$ (line 14). If $e m d_{\\mathbf{p}}^{+}<\\theta, \\mathbf{p}$ is updated in $H$ and the threshold $\\theta$ is updated accordingly (line 18). After $\\theta$ is updated, we remove from $P L$ objects whose lower bounds are already greater than or equal to $\\theta$ (line 19). The removal is facilitated by an additional heap structure that indexes objects in descending order of their lower bounds. If, after some augmentations, $e m d_{\\mathbf{p}}^{+}$becomes no smaller than $P L . t o p() . e m d^{-}$(line 11) and $\\mathbf{p}$ has not been pruned, it is put back to $P L$ (line 23) and the top object of $P L$ takes its place in the inner path augmentation loop. Note that objects that may not be further refined (i.e., condition $e m d_{\\mathbf{p}}^{+}=e m d_{\\mathbf{p}}^{+}$at line 13) are either pruned (if $e m d_{\\mathbf{p}} \\geq \\theta$ ) or added to $H$ (if $e m d_{\\mathbf{p}}<\\theta$ ).\n![img-9.jpeg](img-9.jpeg)\n\nFigure 10: Prioritizing refinement order\nFigure 10 illustrates a running instance of DRO. The current lower and upper bounds of each object are indicated by the borders of a bar. Suppose $k=3$ and there are 7 objects $(=b)$ in $P L$.\n\n\n[^0]:    ${ }^{b}$ For the ease of presentation, we denote $e m d^{-}(\\mathbf{q}, \\mathbf{p})$ by $e m d_{\\mathbf{p}}^{-}$, as the query histogram $\\mathbf{q}$ is the same for all objects in $\\mathcal{D}$.\n\nNote that the currently best object in $P L$ (in terms of $e m d^{-}$) is $\\mathbf{p}_{4}$, so DRO refines the current EMD of $\\mathbf{p}_{4}$ by augmenting flows. After the augmentation, the EMD bounds of $\\mathbf{p}_{4}$ are updated to the shaded bar. Note that $e m d_{\\mathbf{p}_{5}}^{-}$now becomes the lowest $e m d^{-}$, so the refinement of $\\mathbf{p}_{4}$ is stalled; in the next iteration, $\\mathbf{p}_{5}$ is de-heaped from $P L$ and refined. After augmenting flows to the EMD of $\\mathbf{p}_{5}$, the EMD bounds of $\\mathbf{p}_{5}$ are updated as shown by the shaded bar. Observe that now $e m d_{\\mathbf{p}_{5}}^{-}<\\theta$; this causes (i) $\\mathbf{p}_{5}$ to be included in the currently best $k$ objects $H$ and (ii) $\\theta$ to be updated to $e m d_{\\mathbf{p}_{2}}^{-}$.\n\nDRO is expected to be more efficient than considering the candidates that pass the filters one by one and computing their exact EMD individually. Concurrent refinement by prioritizing candidates with the lowest $e m d^{-}$performs the first (cheap) iterations of SIA for many candidates and helps in deriving upper bounds $e m d^{+}$ for them and a good estimate of $\\theta$ early. Obtaining a tight $\\theta$ early can help (i) to prune more candidates using the filters (function getnext_filter) and (ii) to avoid the late and expensive path augmentations of SIA for many candidates in $P L$ that can be pruned.\n\nThe choice of $b$ (i.e., the size of $P L$ ) affects the performance of DRO; if $b$ is very large, DRO performs multiple concurrent SIA executions which may have high memory requirements. If $b$ is too small, the pruning threshold $\\theta$ does not converge fast to its final value, and more objects enter the refinement phase. Clearly, there is a tradeoff between the performance gain and memory consumption. In our experiments, every SIA thread only constructs a small portion of the entire flow graph. Thereby, $b$ can be set to a relatively large number. For instance, when $b$ is set to $0.2 \\%$ of the data cardinality, the peak memory consumption of DRO on the largest dataset of our experimental evaluation, WORLD, is just around 20 times of a complete flow graph in a single EMD computation. Moreover, we study a mechanism to avoid worst cases (e.g., every partial graph in $P L$ is full). When the size of $P L$ exceeds a limit, we stop inserting candidates into $P L$ and refine the current EMD by PB until it is pruned or becomes a $k$-NN candidate. However, in all of our experimental testings, this mechanism is never triggered as the size of $P L$ is much smaller than our default memory limit (i.e., 512 MB ).\n\n### 4.5 Running Upper Bound\n\nWe now discuss the details of computing and maintaining a running upper bound $e m d_{\\mathbf{p}}^{+}$for an object $\\mathbf{p}$ whose EMD has partially been computed by SIA. The upper bounds are used by DRO (presented in Section 4.4) to derive a value for $\\theta$ after having partially computed the EMD of some candidate objects. According to the EMD definition, any flow matrix $\\mathbf{F}$ satisfying all three conditions of Equation 2 must lead to an upper bound of the actual EMD. For the ease of discussion, we call a flow matrix $\\mathbf{F}$ possible if it satisfies all these three optimization constraints; the constraints are satisfied if and only if there is no more feasible path in the flow network. Thus, finding a possible flow matrix $\\mathbf{F}$ is equivalent to finding a maximum flow in the network [1], disregarding edge costs.\n\nBased on the above discussion, we can define a running upper bound $e m d^{+}$following the same idea of deriving $e m d^{-}$. Similar to the illustration in Figure 8, $e m d^{+}$consists of the current cost plus an upper bound considering the nodes which still have remaining flow capacity (i.e., the vertices inside the dashed-line regions). For instance, a maximum flow of the vertices inside the dashed-line region is $\\left(q_{1}, p_{4}, 1\\right),\\left(q_{2}, p_{4}, 1\\right)$, and $\\left(q_{4}, p_{4}, 1\\right)$. The cost of this max flow is $0.7+0.9+0=1.6$; thus, $e m d^{+}$is 1.2 (current cost) + $1.6=2.8$. Formally, we define the running upper bound as follows.\n\nDefinition 4 (Running Upper Bound, $e m d^{+}$).\nConsider a flow graph, such that for each node $p_{i} \\in \\mathbf{p}$, there is a set of $m_{i}$ augmented paths $s p_{q_{i}, 1}, \\ldots, s p_{q_{i}, m_{i}}$. The\nrunning upper bound $e m d^{+}$is defined as\n\n$$\n\\begin{aligned}\ne m d^{+}= & \\sum_{q_{j} \\in \\mathbf{q}} \\sum_{j=1}^{m_{i}} c_{f}\\left(s p_{q_{i}, j}\\right) \\cdot f\\left(s p_{q_{i}, j}\\right)+ \\\\\n& \\max f l o w(\\mathbf{q}, \\mathbf{p})\n\\end{aligned}\n$$\n\n(maximum flow)\nwhere $\\max f l o w(\\mathbf{q}, \\mathbf{p})$ returns the cost of a maximum flow based on the remaining capacities of $\\mathbf{q}$ and $\\mathbf{p}$.\n\nTo compute $e m d^{+}$, we can apply any maximum flow algorithm; however, existing maximum flow algorithms are too expensive [1], considering the fact that $e m d^{+}$should be maintained throughout the entire EMD calculation. Instead of using these methods, we propose an efficient greedy approach, which takes advantage of the EMD flow network topology; note that the EMD flow graph is a complete bipartite graph between the bins of $\\mathbf{q}$ and $\\mathbf{p}$. This means that a feasible vertex of $\\mathbf{q}$ can always augment flow to any feasible vertex of $\\mathbf{p}$ along one edge. Our GreedyUB algorithm (Algorithm 4), for each feasible vertex $q_{i}$ of $\\mathbf{q}$, accesses the feasible vertices $p_{j}$ of $\\mathbf{p}$ in increasing order of $c_{i, j}$ and augments the maximum possible flow along each edge $e\\left(q_{i}, p_{j}\\right)$, until the remaining capacity from $q_{i}$ has been used up. The accumulated cost of the greedily augmented flows is used as the maxflow component $\\max f l o w(\\mathbf{q}, \\mathbf{p})$ of $e m d^{+}(\\mathbf{q}, \\mathbf{p})$.\n\n```\nAlgorithm 4 Greedy Upper Bound\n    Algorithm GreedyUB(Histograms q, p)\n    cost \\(:=0\\)\n    while \\(\\exists\\) feasible \\(q_{i}\\) do\n        while \\(\\operatorname{cap}_{q_{i}}>\\bar{f}_{q_{i}}\\) do \\(\\triangleright q_{i}\\) has remaining capacity\n            \\(p_{j}:=\\) feasible bin in \\(\\mathbf{p}\\) having minimum \\(c_{i, j}\\)\n            flow \\(:=\\min \\left\\{\\operatorname{cap}_{q_{i}}-f_{q_{i}}, \\operatorname{cap}_{p_{j}}-f_{p_{j}}\\right\\}\\)\n            \\(f_{q_{i}}:=f_{q_{i}}+f l o w\\)\n            \\(f_{p_{i}}:=f_{p_{i}}+f l o w\\)\n            \\(\\operatorname{cost}:=\\operatorname{cost}+c_{i, j} \\cdot f l o w\\)\n    return cost\n```\n\nTo facilitate GreedyUB, for each row $i$ of the cost matrix $\\mathbf{C}$, we define an order for the bins $j$ based on $c_{i, j}$ (this order is static, i.e., independent of the data and queries, and it is defined once, together with the ground distance function or cost matrix). For example, for row $q_{1}$ of the matrix shown in Table 1, the order is $\\left\\{p_{1}, p_{3}, p_{4}, p_{2}\\right\\}$. Then, in Algorithm 4 line 4, this order is considered for identifying feasible bins $p_{j}$ for the current vertex $q_{i}$. Thus, the complexity of the greedy algorithm is $O(|E|)$, where $E$ indicates the edges in the graph, since there are at most $|E|$ feasible pairs in the EMD flow graph, i.e., much lower compared to the complexity of standard maximum flow algorithms (e.g., $O(|V||E||)$.\nSimilar to $e m d^{-}$, we can update $e m d^{+}$incrementally after each path augmentation during the refinement of an EMD. After every augmentation of SIA, according to the capacity feasibility, we cancel and re-augment (for those that become feasible after the cancelation) the corresponding flows in the maximum flow graph formed by the greedy algorithm. The number of canceledtre-augmented edges in the maximum flow instance is at most equal to the flow units $f$ just being augmented in the current EMD flow. Thereby, the maintenance cost of $e m d^{+}$after each augmentation is $O(f)$.\n\n## 5. EXPERIMENTAL EVALUATION\n\nIn this section, we conduct extensive experiments to evaluate the performance of our proposed EMD-based similarity search framework and compare it with state-of-the-art solutions. All methods are implemented in C++ and evaluated on 3.40 GHz quad-core machines running Ubuntu 12.04, with 16 GBytes of main memory.\n\nTable 3: The statistics of six real datasets\n\n| Name | \\# Objs | \\# Bins | Description |\n| :--: | :--: | :--: | :--: |\n| RETINA | 3932 | $\\begin{gathered} 96 \\\\ (12 \\times 8) \\end{gathered}$ | Feline retina images. Default dataset in $[25,30,32]$. |\n| IRMA | 10 K | 199 | Medical images from IRMA ${ }^{7}$ project. Used in $[25,30,32]$. |\n| FLICKR | 680 K | $\\begin{gathered} 100 \\\\ (10 \\times 10) \\end{gathered}$ | Images crawled from Flickr ${ }^{8}$. This dataset is used in [25]. |\n| PANORAMIO | 500 K | $\\begin{gathered} 576 \\\\ (24 \\times 24) \\end{gathered}$ | Images of European cities from Panoramio ${ }^{9}$. |\n| FRIENDS | 320 K | $\\begin{gathered} 768 \\\\ (24 \\times 32) \\end{gathered}$ | Images captured every 25 frames from the TV series \"Friends\". |\n| WORLD | 3 M | $\\begin{gathered} 1024 \\\\ (32 \\times 32) \\end{gathered}$ | Images from ImageNet ${ }^{10}$ project. |\n\nWe use six real datasets in our evaluation; their default statistics are shown in Table 3. The RETINA, IRMA and FLICKR histogram sets are taken from [25]. The PANORAMIO, FRIENDS and WORLD histogram sets are generated by the same method with RETINA and FLICKR as suggested in [25]: each image is divided into tiles by a grid (e.g., $24 \\times 24$ square granularity); for each tile, the 12 -feature MPEG-7 color layout descriptor (CLD) is extracted, and we pick only the first feature as the value of the histogram bin. Similar to $[25,30,32]$, we use Euclidean distance as the ground distance (cost matrix) in all the experiments. Note that our techniques are not restricted to any specific ground distance.\n\nIn the evaluation, we compare a set of EMD computation methods when used as a black-box module of the filter-and-refinement framework (see Section 2.2). In all cases, the filtering of candidates is done by applying the state-of-the-art filtering techniques as used in [25]. First, we use the normal distribution index [25] to implement the I.getnext function. For each retrieved candidate by this function, we apply the following set of filters $\\Delta$ in this order: full projection [7], reduced dimension [30] (only for RETINA and IRMA, on which the filter has positive effects as in [25]), and independent minimization [5]. For the candidates that pass the filter phase, we compare the application of the following EMD computation methods: (i) capacity scaling (CAP), (ii) cost scaling (COS), (iii) transportation simplex (TRA), (iv) network simplex (NET), (v) SSP (Section 2.1), (vi) SIA (Section 3). In addition, we evaluate our progressive bounding ( PB ) and dynamic refinement ordering (DRO) techniques when applied in conjunction with SIA. At each experimental instance (e.g., for a given dataset and $k$ ), we run 100 $k$-NN queries choosing $\\mathbf{q}$ randomly from the corresponding dataset and average the query cost. The 100 queries for RETINA, IRMA and FLICKR dataset are the same queries used in [25, 30, 32]. The default value of $k$ is 32 and the default $b$ (i.e., maximum size of $P L$ in DRO) is set to $0.2 \\%$ of the corresponding dataset cardinality.\n\n### 5.1 Performance Improvement\n\nFirst, we compare the performance of different black-box EMD computation methods in the filter-and-refinement framework. Table 4 shows the average query time of the six EMD computation methods for 32-NN similarity queries on the six datasets. SIA is the best method which constantly outperforms its base method SSP and other alternatives (e.g., TRA) by at least 2.4 times. In addition, SIA scales well with the problem dimensionality (i.e., the number of histogram bins and the cardinality of datasets). Figure 11 shows\n\n[^0]Table 4: Comparison of black-box EMD computation methods\n\n|  |  | RETINA | IRMA | FLICKR | PANO | FRIENDS | WORLD |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | CAP | 0.68 s | 3.90 s | 7.89 s | 803 s | 3279 s | 22715 s |\n|  | COS | 1.13 s | 6.48 s | 13.58 s | 1394 s | 5628 s | 48917 s |\n|  | NET | 0.52 s | 4.13 s | 7.04 s | 763 s | 3573 s | 22774 s |\n|  | SSP | 0.57 s | 6.14 s | 7.52 s | 2048 s | 10370 s | 65731 s |\n|  | TRA | 0.57 s | 8.30 s | 7.72 s | 3395 s | 15307 s | 137414 s |\n|  | SIA | 0.17 s | 1.51 s | 2.77 s | 318 s | 1362 s | 7082 s |\n| Filter time |  | 0.006 s | 0.008 s | 0.749 s | 1.009 s | 1.476 s | 12.238 s |\n\nthe performance of the methods as a function of $k$ on five datasets: RETINA, IRMA, PANORAMIO, FRIENDS, and WORLD. ${ }^{11}$ SIA outperforms SSP by around an order of magnitude, while being at least two times faster than the runner-up network simplex (NET). Note that the relative performance difference between methods is not very sensitive to $k$. SIA is obviously the best module for calculating EMD under the filter-and-refinement framework. Note also that the time spent in the filter phase (in Table 4) is negligible, which confirms our discussion that the refinement phase dominates the runtime of the entire filter-and-refinement framework.\n\nNext, we evaluate the improvement offered by our progressive bounding and dynamic refinement ordering techniques. Figure 12 compares the runtime of $k$-NN queries when (i) using SIA as a black-box module, (ii) applying progressive bounding in SIA (PB), and (iii) applying dynamic refinement ordering in conjunction with progressive bounding in SIA (DRO). Observe that PB achieves a large performance improvement over SIA (i.e., the best black-box module in Figure 11). The improvement generally becomes more visible when the problem size increases (i.e., the number of bins and the cardinality of datasets). This can be explained by the fact that the size of the flow graph is quadratic to the number of histogram bins, therefore the latter iterations of SIA become extremely expensive. The progressive bounding technique helps to avoid reaching these iterations, as explained in Section 4.2. DRO offers a stable improvement over $\\mathrm{PB}(40 \\%-60 \\%)$, indicating that the concurrent refinement and dynamic reordering techniques have positive impact on the performance.\n\nIn summary, DRO is the recommended methodology for EMDbased similarity queries, being several times to two orders of magnitude faster than using off-the-shelf EMD computation methods. Notably, all previous works $[5,25,30,32]$ adopt transportation simples (TRA) for calculating EMD at the refinement phase. As shown in our evaluation, this method only performs well when the number of bins is relatively small due to its exponential time complexity.\nRoom for Improvement. So far, we have shown the superiority of our PB and DRO techniques over existing approaches. To see how much room for improvement exists for EMD-based similarity queries, we compare PB and DRO to two ideal yet practically infeasible methods, ESS and OI. Assuming that we know the result of a $k$-NN query by an oracle, ESS (Essential $k$-NN search) calculates the EMDs of only the exact $k$-NNs, representing the lowest possible effort to compute the results of a $k$-NN query. OI (Oracle Index) assumes that an oracle index is available which offers an optimal getnext function that returns the objects in the ascending order of their exact EMDs. OI can be viewed as an optimal ordering method integrated with the current state-of-the-art filtering techniques. In our experiments, both ESS and OI calculate EMDs by the best black-box module (SIA), and OI uses our progressive bounding (PB) technique to terminate EMD calculations early.\n\nFigure 13 shows the query time of four methods, PB, DRO, ESS, and OI, as a function of $k$. First, we observe that DRO's perfor-\n\n[^1]\n[^0]:    ${ }^{7}$ http://ganymed.imib.rwth-aachen.de/irma\n    ${ }^{8}$ http://www.flickr.com\n    ${ }^{9}$ http://www.panoramio.com\n    ${ }^{10} \\mathrm{http}: / /$ www.image-net.org\n\n[^1]:    ${ }^{11}$ The results on FLICKR are similar to those of RETINA.\n\n![img-10.jpeg](img-10.jpeg)\n\nFigure 11: Performance of black-box EMD computation methods varying on $k$\n![img-11.jpeg](img-11.jpeg)\n\nFigure 12: Performance of SIA, PB and DRO varying on $k$\n![img-12.jpeg](img-12.jpeg)\n\nFigure 13: Closeness of PB and DRO to the oracle methods (query time)\n![img-13.jpeg](img-13.jpeg)\n\nFigure 14: Closeness of PB and DRO to the oracle methods (number of refinements)\nmance is very close to that of OI, improving PB by a significant extent towards an ideal method. Figure 14 illustrates the number of EMD refinements ${ }^{12}$ of these four methods, and also the number of complete refinements (i.e., the EMD calculations that are fully conducted) of PB and DRO (denoted by PB-C and DRO-C respectively), varying on $k$. These experiments confirm the robustness of our dynamic reordering technique as DRO commences almost the same number of EMDs compared to the optimal method OI. Besides, in Figure 14, PB-C and DRO-C are very close to ESS (i.e., $k$ ). In specific, DRO-C is only slightly larger than $k$, indicating that only very few objects that are not the actual $k$-NN results need to be fully refined by DRO, which again verifies the effectiveness of DRO. Obviously, there is limited room for improving DRO, using the current state-of-the-art filtering techniques, as its performance is already close to that of OI. On the other hand, we believe that there is still room for improving upon the current filtering methods as the performance gap between OI and ESS is still significant.\n\n[^0]\n### 5.2 Scalability Experiments\n\nIn the first scalability study, we conduct experiments on PANORAMIO subsets with cardinality 100 K to 500 K , on FRIENDS subsets ( 64 K to 320 K ), and on WORLD subsets ( 0.6 M to 3 M ), by randomly picking objects from the corresponding datasets. The lines in Figure 15 show the query time of three methods, SIA, PB, and DRO, as a function of the object cardinality, after setting $k=32$. PB and DRO scale very well with the database size (they are almost insensitive), since the progressive bounding technique benefits from better pruning thresholds owing to the increasing number of objects. This experiment exposes an important advantage of our approach: its performance is less sensitive to the database size, while $k$ is the main cost factor. Note that while the database size may increase arbitrarily, in practice $k$ is not expected to grow at the same rate, since similarity search queries typically retrieve a limited number of objects. The bars in Figure 15 show the number of refinements of SIA, PB, and DRO (note that in PB and DRO, only a portion of these are complete refinements, denoted\n\n\n[^0]:    ${ }^{12}$ An object is counted even when only one path is augmented.\n\n![img-14.jpeg](img-14.jpeg)\n\nFigure 15: Scalability to dataset cardinality (number of objects)\nby $\\mathrm{PB}-\\mathrm{C}$ and DRO-C). We observe that SIA is quite sensitive to the number of refinements while PB and DRO are likely sensitive to the number of the complete refinements ( $\\mathrm{PB}-\\mathrm{C}$ and DRO-C). In addition, the improvement of DRO over PB is stable, indicating that the reordering technique is not affected by the increasing number of objects that survive the filters.\n\nIn the second scalability study, we evaluate the effectiveness of our techniques against the histogram dimensionality (i.e., number of bins). We generate histogram sets of different dimensionalities (square grid granularities) up to 2304, on PANORAMIO and WORLD. Figure 16 shows the query time as a function of the histogram dimensionality on the two datasets $(k=32)$. We observe that the query time of PB and DRO increases more slowly compared to that of SIA. The graceful scalability of our methods w.r.t. histogram dimensionality enables the application of EMDbased similarity search on high-dimensional histogram representations (e.g., multi-dimensional features with fine partitioning).\n\n### 5.3 Parameter Tuning in DRO\n\nFinally, we demonstrate the effect of two sensitive parameters in DRO, i.e., the maximum number of concurrently refined objects $b$ (the size of $P L$ ), and the function to prioritize objects in DRO.\n![img-15.jpeg](img-15.jpeg)\n\nFigure 17: Performance tuning of DRO on PANORAMIO\nFigure 17(a) shows the average query time of $k$-NN queries on the PANORAMIO dataset, for different values of $b$ (as different series). DRO becomes faster when $b$ increases, which indicates that concurrent refinement is an effective strategy to evaluate EMDbased $k$-NN queries. On the other hand, as discussed in Section 4.4, the memory requirements of DRO are proportional to $b$, so the performance improvement comes with a memory tradeoff.\n\nAs mentioned in Section 4.4, the running lower bound $e m d^{-}$ can be used to prioritize objects in DRO. However, as shown in Figure 17(b), $e m d^{-}$is not an effective prioritizing function since it does not take the bound tightness into consideration. For instance, assume that both $e m d_{\\mathbf{p}_{\\mathbf{a}}}^{-}$and $e m d_{\\mathbf{p}_{\\mathbf{b}}}^{-}$have the same value. Intuitively, if $e m d_{\\mathbf{p}_{\\mathbf{a}}}^{+}<e m d_{\\mathbf{p}_{\\mathbf{b}}}^{+}, \\mathbf{p}_{\\mathbf{a}}$ should be given higher priority than $\\mathbf{p}_{\\mathbf{b}}$ during the refinement, because $\\mathbf{p}_{\\mathbf{a}}$ has a tighter bound and it is more likely to be added to the current $k$-NN set $H$ and decrease $\\theta$. Thus, DRO (in Figure 17(b) and the previous experiments) uses the following prioritizing function, which is a slight modification of $e m d^{-}$(Definition 3): $(1-\\alpha)$-current cost $+\\alpha$-estimated\ncost, where $\\alpha$ indicates the tightness of the running bounds (i.e., $\\frac{e m d^{+}-e m d^{-}}{e m d^{+}}$). As Figure 17(b) shows, this function, when used in DRO, maintains a constant advantage over PB, compared to prioritizing by just using $e m d^{-}$or $e m d^{+}$.\n\n## 6. RELATED WORK\n\nEarth Mover's Distance (EMD) was first introduced by the computer vision community as an effective similarity metric [23]. EMD is also known as Mallows distance or Wasserstein distance in statistics [15]. As a cross-bin distance, EMD matches better the human perception of differences [22], compared to bin-by-bin distances like Euclidean distance or $\\chi^{2}$-statistic. EMD supports analysis and search in a wide range of application domains, such as image retrieval [23,24], computer vision [11,17,21], machine learning [6,9], probabilistic $[25,32]$ and multimedia databases [5, 30], video and music identification [28, 31], phishing detection [10], data cleaning [8], privacy [16], matrix factorization [26], clustering [4, 6], classification [14], etc. An empirical study [20] that compares nine families of image dissimilarity measures based on distributions of color and texture features shows that EMD has the best overall quality among the others, yet also the highest computational cost.\n\nDue to the usefulness of EMD, the database community developed techniques for similarity queries based on this measure. Work in this direction is based on the filter-and-refinement framework $[5,25,30,32]$, described in Section 2.2. The main focus is the development of fast and effective lower bounds for EMD, which help in pruning objects that cannot make it in the result, at the filter phase. Assent et al. [5] were the first to study EMD in the filter-and-refinement framework. Wichterich et al. [30] propose a dimensionality reduction technique, showing that the EMD of two objects in the derived space is a lower bound for their actual EMD. Xu et al. [32] propose a lower bound of EMD based on primal-dual techniques [1] from linear programming and use $\\mathrm{B}^{+}$-tree indexing to support the filter phase of $k$-NN and range queries. Ruttenberg and Singh [25] develop a new index structure for EMD-based similarity queries based on the projection lower bound in [7]. All these solutions focus only on the effectiveness of filters and efficiency of the filter phase, simply treating the refinement phase as a black-box process. However, as we have shown in this paper, the refinement cost dominates the overall cost and deserves more attention.\n\nBesides these results by the database community, there are also studies on efficient EMD approximations [2, 3, 13, 19, 27]. Andoni et al. [3] study EMD approximations in high-dimensional spaces. Pele and Werman [19] accelerates the EMD computation by limiting the number of edges in the flow graph, without guarantees of the quality of approximation. Both [13] and [27] study linear-time approximations of EMD, and [2] proposes sketches for approximating planar EMD. However, these works assume either specific ground distances (e.g., $L_{1}$ norm) [2, 3, 27] and bin spaces (e.g., $\\mathbb{R}^{2}$ ) [2, 3, 13, 27], or certain histogram types (e.g., dominant color descriptor) [13], which do not apply to the general setup of EMD.\n\nFor example, the choice of ground distance could be applicationdependent $[19,22,24]$. Note that our proposed techniques do not have any of the above restrictions. Moreover, there is no study on filtering bounds and indexing structures on top of these approximation techniques. Therefore, using the approximation methods, a similarity query may have to compute EMDs between the query and all the objects in the database, which is not scalable for large datasets. Finally, the acceleration of EMD under Manhattan network when using $L_{1}$ as the ground distance is studied in [18].\n\n## 7. CONCLUSION\n\nIn this paper, we studied the efficient evaluation of similarity queries using Earth Mover's Distance (EMD). First, we showed how we can adapt SIA, an algorithm originally proposed for spatial matching problems, to compute the EMD between two histograms efficiently. Then, we proposed a progressive refinement strategy, which updates a lower bound for EMD during its computation, in order to abandon early a partial EMD refinement, if the object cannot make it in the query result. Finally, we proposed a technique which concurrently handles the refinement of multiple candidates, by dynamically reordering them and computing upper bounds that help to tighten the pruning threshold early. Our experiments show that our methods are very effective in practice, decreasing the overall cost of EMD-based similarity queries by up to two orders of magnitude, compared to the state-of-the-art solution [25].\n\nNote that although our discussion assumes that the histograms are normalized to sum up to the same values, EMD can also be applied in cases where histograms are not normalized (in this case, a slightly different definition than that of Equation 2 is used to reflect the maximum flow that could be sent from $\\mathbf{q}$ to $\\mathbf{p}$ ). In addition, it is not necessary that the two histograms have the same number or locations of bins. Our framework and solutions are not sensitive to these problem variants. Finally, note that our solution can also be used for evaluating range similarity queries, where the objective is to retrieve objects whose EMD to $\\mathbf{q}$ does not exceed a given threshold $\\epsilon$. In this case, SIA and PB can directly be applied, however, DRO is not relevant because the threshold is fixed and insensitive to the execution order.\n\nIn the future, we plan to study the optimization of the refinement step in similarity queries based on other expensive distance measures (such as dynamic time warping).\n\n## 8. ACKNOWLEDGMENT\n\nThis work is supported by grants HKU 714212E, 711110, and 711309E from Hong Kong RGC. Leong Hou U is supported by grant MYRG109(Y1-L3)-FST12-ULH. We would like to thank Brian Ruttenberg for providing part of his code on [25] and the anonymous reviewers for their insightful comments.\n\n## 9. REFERENCES\n\n[1] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin. Network flows: theory, algorithms and applications. Prentice Hall, 1993.\n[2] A. Andoni, K. D. Ba, P. Indyk, and D. P. Woodruff. Efficient sketches for earth-mover distance, with applications. In FOCS, pages $324-330,2009$.\n[3] A. Andoni, P. Indyk, and R. Krauthgamer. Earth mover distance over high-dimensional spaces. In SODA, pages 343-352, 2008.\n[4] D. Applegate, T. Dasu, S. Krishnan, and S. Urbanek. Unsupervised clustering of multidimensional distributions using earth mover distance. In $K D D$, pages 636-644, 2011.\n[5] I. Assent, A. Wenning, and T. Seidl. Approximation techniques for indexing the earth mover's distance in multimedia databases. In ICDE, page 11, 2006.\n[6] M. H. Coen, M. H. Ansari, and N. Fillmore. Comparing clusterings in space. In ICML, pages 231-238, 2010.\n[7] S. Cohen and L. Guibas. The earth mover's distance: Lower bounds and invariance under translation. Technical report, Stanford University, 1997.\n[8] T. Dasu and J. M. Loh. Statistical distortion: Consequences of data cleaning. PVLDB, 5(11):1674-1683, 2012.\n[9] N. Ferns, P. S. Castro, D. Precup, and P. Panangaden. Methods for computing state similarity in markov decision processes. In UAI, pages $174-181,2006$.\n[10] A. Y. Fu, L. Wenyin, and X. Deng. Detecting phishing web pages with visual similarity assessment based on earth mover's distance (EMD). IEEE Trans. Dependable Sec. Comput., 3(4):301-311, 2006.\n[11] K. Grauman and T. Darrell. Fast contour matching using approximate earth mover's distance. In CVPR (1), pages 220-227, 2004.\n[12] F. S. Hillier and G. J. Lieberman. Introduction to Mathematical Programming. McGraw-Hill, 1990.\n[13] M.-H. Jang, S.-W. Kim, C. Faloutsos, and S. Park. A linear-time approximation of the earth mover's distance. In CIKM, pages $505-514,2011$.\n[14] H. J. Karloff, S. Khot, A. Mehta, and Y. Rabani. On earthmover distance, metric labeling, and 0 -extension. In STOC, pages 547-556, 2006.\n[15] E. Levina and P. J. Bickel. The earth mover's distance is the mallows distance: Some insights from statistics. In ICCV, pages 251-256, 2001.\n[16] N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond k-anonymity and l-diversity. In ICDE, pages 106-115, 2007.\n[17] P. Li, Q. Wang, and L. Zhang. A novel earth mover's distance methodology for image matching with gaussian mixture models. In ICCV, 2013 (to appear).\n[18] H. Ling and K. Okada. An efficient earth mover's distance algorithm for robust histogram comparison. IEEE Trans. Pattern Anal. Mach. Intell., 29(5):840-853, 2007.\n[19] O. Pele and M. Werman. Fast and robust earth mover's distances. In ICCV, pages 460-467, 2009.\n[20] J. Puzicha, Y. Rubner, C. Tomasi, and J. M. Buhmann. Empirical evaluation of dissimilarity measures for color and texture. In ICCV, pages 1165-1172, 1999.\n[21] Z. Ren, J. Yuan, and Z. Zhang. Robust hand gesture recognition based on finger-earth mover's distance with a commodity depth camera. In ACM Multimedia, pages 1093-1096, 2011.\n[22] Y. Rubner and C. Tomasi. Perceptual Metrics for Image Database Navigation. Kluwer Academic Publishers, 2001.\n[23] Y. Rubner, C. Tomasi, and L. J. Guibas. A metric for distributions with applications to image databases. In ICCV, pages 59-66, 1998.\n[24] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover's distance as a metric for image retrieval. International Journal of Computer Vision, 40(2):99-121, 2000.\n[25] B. E. Ruttenberg and A. K. Singh. Indexing the earth mover's distance using normal distributions. PVLDB, 5(3):205-216, 2011.\n[26] R. Sandler and M. Lindenbaum. Nonnegative matrix factorization with earth mover's distance metric. In CVPR, pages 1873-1880, 2009.\n[27] S. Shirdhonkar and D. W. Jacobs. Approximate earth mover's distance in linear time. In CVPR, pages 1-8, 2008.\n[28] R. Typke, P. Giannopoulos, R. C. Veltkamp, F. Wiering, and R. van Oostrum. Using transportation distances for measuring melodic similarity. In ISMIR, pages 107-114, 2003.\n[29] L. H. U, K. Mouratidis, M. L. Yiu, and N. Mamoulis. Optimal matching between spatial datasets under capacity constraints. ACM Trans. Database Syst., 35(2), 2010.\n[30] M. Wichterich, I. Assent, P. Kranen, and T. Seidl. Efficient EMD-based similarity search in multimedia databases via flexible dimensionality reduction. In SIGMOD, pages 199-212, 2008.\n[31] J. Xu, Q. Bai, Y. Gu, A. K. H. Tung, G. Wang, G. Yu, and Z. Zhang. EUDEMON: A system for online video frame copy detection by earth mover's distance. In ICDE, pages 1233-1236, 2012.\n[32] J. Xu, Z. Zhang, A. K. H. Tung, and G. Yu. Efficient and effective similarity search over probabilistic data based on earth mover's distance. PVLDB, 3(1):758-769, 2010.\nOutput:",
      "name": null
    },
    {
      "role": "assistant",
      "content": "```json\n{\n    \"meta\": {\n        \"poster_title\": \"Earth Mover's Distance based Similarity Search at Scale\",\n        \"authors\": \"Yu Tang, Leong Hou U, Yilun Cai, Nikos Mamoulis, Reynold Cheng\",\n        \"affiliations\": \"The University of Hong Kong, University of Macau\"\n    },\n    \"sections\": [\n        {\n            \"title\": \"Poster Title\",\n            \"content\": \"Earth Mover's Distance based Similarity Search at Scale\\n\\nYu Tang, Leong Hou U, Yilun Cai, Nikos Mamoulis, Reynold Cheng\\nThe University of Hong Kong, University of Macau\"\n        },\n        {\n            \"title\": \"Introduction & Motivation\",\n            \"content\": \"Earth Mover's Distance (EMD) is a robust similarity measure for multidimensional histograms, offering better distinguishability than classic measures like Euclidean distance. It is widely used in multimedia databases, computer vision, and machine learning. However, EMD's high computational cost, typically requiring super-cubic time, restricts its scalability. EMD-based similarity searches usually adopt a filter-and-refinement framework. While extensive research has focused on creating tighter lower bounds to prune more candidates in the filter step, the refinement step remains the performance bottleneck. For large-scale datasets, even with a high filtering ratio (e.g., 99%), the cost of exact EMD calculations for the remaining candidates is prohibitive. This paper focuses on optimizing the refinement phase. We propose three key techniques: (i) adapting an efficient min-cost flow algorithm, Simplified Graph Incremental Algorithm (SIA), for EMD computation, (ii) a progressive bounding (PB) technique that uses a dynamic distance bound to terminate EMD calculations early, and (iii) a dynamic refinement ordering (DRO) technique that concurrently refines multiple candidates to reduce needless computations. These techniques are orthogonal to existing filtering methods and can reduce similarity query costs by orders of magnitude.\"\n        },\n        {\n            \"title\": \" Preliminaries\",\n            \"content\": \"Given two normalized histograms, q and p, the Earth Mover's Distance (EMD) is the minimum cost to transform one into the other. It is modeled as a min-cost flow (MCF) problem on a bipartite network where vertices represent histogram bins. The cost of moving 'earth' between bins is given by a ground distance matrix. The Successive Shortest Path (SSP) algorithm is a common method for solving MCF. SSP iteratively finds the minimum-cost feasible path in the flow network and augments the maximum possible flow along it. This process repeats until all flow from q has been moved to p. The complexity of SSP is high, requiring O(F|E| log |V|) time, where F is the total flow. A key challenge is that each path augmentation can alter the graph, requiring repeated shortest path searches on the complete bipartite graph, which is quadratic to the number of histogram bins.\\n\\nEMD similarity queries are typically evaluated using a filter-and-refinement framework. In the filter phase, cheap-to-compute lower bounds of the EMD are used to prune unpromising candidates. For a k-NN query, a pruning threshold  is maintained, representing the distance to the current k-th nearest neighbor. Any object whose EMD lower bound exceeds  is filtered. Objects that survive the filter phase proceed to the refinement step, where their exact EMD is computed. This refinement step is the bottleneck we aim to optimize.\"\n        },\n        {\n            \"title\": \"Scaling EMD Computation\",\n            \"content\": \"To address the high cost of EMD computation, we adapt the Simplified Graph Incremental Algorithm (SIA), which is an optimized version of SSP. Unlike SSP, which operates on the complete bipartite graph, SIA incrementally constructs a much smaller partial flow graph. It begins with an empty graph and adds edges in ascending order of their costs as needed. At each step, SIA finds the shortest path in the current partial graph. A distance bound, derived from the costs of edges not yet in the partial graph, is used to guarantee that the path found in the partial graph is globally optimal. If the current shortest path's cost is less than this bound, the path is valid and flow is augmented. Otherwise, SIA expands the partial graph by adding the next cheapest edge and repeats the search. This incremental approach significantly reduces the search space and computational cost, as the partial graph is typically much smaller than the complete one, making it highly scalable for histograms with a large number of bins.\"\n        },\n        {\n            \"title\": \"Progressive Bounding (PB)\",\n            \"content\": \"Although SIA is efficient for a single EMD calculation, we observe that over 90% of its execution time is spent in the last 20% of its iterations, as the partial graph grows and shortest path searches become expensive. To exploit this, we introduce Progressive Bounding (PB), a technique to terminate SIA early for candidates that will not make it into the final k-NN result. We define a running lower bound, emd-, which is updated throughout the SIA process. This bound consists of two parts: the actual cost of the flow augmented so far, and an estimated cost for the remaining flow yet to be augmented. The estimated cost is calculated based on the cost of the most recent augmented path from each source bin, leveraging the property that path costs from a given bin are monotonically non-decreasing. At each iteration of SIA, we update emd- and compare it against the current k-NN pruning threshold . If emd-  , we can safely terminate the EMD calculation for the current candidate, as its final EMD will be no smaller than . This allows us to prune unpromising candidates mid-refinement, avoiding the most computationally expensive final iterations of SIA and drastically reducing overall query time.\"\n        },\n        {\n            \"title\": \"Dynamic Refinement Ordering (DRO)\",\n            \"content\": \"The effectiveness of PB depends on having a tight pruning threshold  early in the query process. However, the standard filter-and-refinement framework processes candidates sequentially, and the order is not optimal for discovering the true nearest neighbors quickly. To address this, we propose Dynamic Refinement Ordering (DRO), a strategy that refines multiple candidates concurrently. DRO maintains a priority layer (PL) of the most promising candidates that have passed the filter step. Instead of fully refining one candidate at a time, DRO interleaves the SIA computations for all candidates in the PL. In each step, it performs a small amount of work (a few path augmentations) on the candidate that is currently most promisingfor instance, the one with the lowest running lower bound emd-. This helps to quickly obtain good estimates for both the lower and upper bounds (emd- and emd+) for many candidates. A tight running upper bound emd+ for a candidate can be used to update and lower the global pruning threshold . A tighter  enables more effective pruning, not only for new candidates in the filter phase but also for the candidates currently being refined in the PL via the PB technique. By dynamically reordering and concurrently processing refinements, DRO converges on the final k-NN set faster and with significantly less overall computation.\"\n        },\n        {\n            \"title\": \"Experimental Results\",\n            \"content\": \"We conducted extensive experiments on six real-world image datasets, with cardinalities up to 3 million and histogram dimensionalities up to 1024 bins. Our evaluation shows that: \\n1. SIA consistently outperforms other black-box EMD computation methods (like SSP and Transportation Simplex) by a significant margin, establishing it as the best baseline for the refinement phase.\\n2. Our Progressive Bounding (PB) technique provides a large performance boost over the baseline SIA. The improvement is more pronounced on larger datasets and higher-dimensional histograms, where the later stages of SIA become extremely expensive.\\n3. Dynamic Refinement Ordering (DRO) offers a stable and significant improvement over PB (40%-60%), demonstrating the effectiveness of concurrent refinement and dynamic reordering. DRO's performance is very close to that of an oracle method with optimal ordering, indicating it is highly effective.\\n4. Compared to state-of-the-art frameworks using standard EMD solvers, our DRO approach is up to two orders of magnitude faster. It scales gracefully with both dataset cardinality and histogram dimensionality, making EMD-based similarity search practical for large-scale, high-dimensional applications.\"\n        },\n        {\n            \"title\": \"Conclusion\",\n            \"content\": \"We proposed a suite of techniques to optimize the bottleneck refinement phase of EMD-based similarity search. By adapting an efficient incremental algorithm (SIA), introducing a progressive bounding method (PB) for early termination, and developing a dynamic refinement ordering strategy (DRO) for concurrent processing, we significantly accelerate similarity queries. Our experimental results confirm that these methods reduce query costs by up to two orders of magnitude compared to the state-of-the-art, enabling scalable and efficient EMD-based search on large, high-dimensional histogram datasets. Our framework is general and can be applied to different EMD problem variants and integrated with existing filtering techniques.\"\n        }\n    ]\n}\n```",
      "name": null
    }
  ]
}