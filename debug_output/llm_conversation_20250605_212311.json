{
  "timestamp": "20250605_212311",
  "model": "gemini-2.5-pro-preview",
  "prompt_version": 2,
  "total_tokens": 19441,
  "input_tokens": 17106,
  "output_tokens": 9864,
  "messages": [
    {
      "role": "system",
      "content": "You are the author of the paper, and you will create a poster for the paper.",
      "name": null
    },
    {
      "role": "user",
      "content": "You are a document content divider and extractor specialist, expert in dividing and extracting content from various types of documents and reorganizing it into a two-level json format for later poster generation.\n\nBased on given markdown document, generate a JSON output for later poster generation, make sure the output is concise and focused.\nStep-by-Step Instructions:\n1. Identify Sections and Subsections in document and identify sections and subsections based on the heading levels and logical structure.\n\n2. Divide Content: Reorganize the content into sections and subsections, ensuring that each subsection contains approximately 500 words.\n\n3. Refine Titles: Create titles for each section with at most 3 words.\n\n4. Remove Unwanted Elements: Eliminate any unwanted elements such as headers, footers, text surrounded by `~~` indicating deletion.\n\n5. Refine Text: For content, you should keep as much raw text as possible. Do not include citations.\n\n6. Length: you should control the length of each section, according to their importance according to your understanding of the paper. For important sections, their content should be long.\n\n7. Make sure there is a poster title section at the beginning, and it should contain information like paper title, author, organization etc.\n\n8. The \"meta\" key contains the meta information of the poster, where the title should be the raw title of the paper and is not summarized.\n\n9. Ther **must** be a section for the poster title.\n\nExample Output:\n{\n    \"meta\": {\n        \"poster_title\": \"raw title of the paper\",\n        \"authors\": \"authors of the paper\",\n        \"affiliations\": \"affiliations of the authors\"\n    },\n    \"sections\": [\n        {\n            \"title\": \"Poster Title & Author\",\n            \"content\": \"content of poster title and author\"\n        },\n        {\n            \"title\": \"title of section1\",\n            \"content\": \"content of section 1\"\n        },\n        {\n            \"title\": \"title of section2\",\n            \"content\": \"content of section 2\"\n        }\n    ]\n}\n\nGive your output in JSON format\nInput:\n# MemO: Building Production-Ready AI Agents with Scalable Long-Term Memory \n\nPrateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav\n\nresearch@mem0.ai\n\n## (1) mem0\n\n\n#### Abstract\n\nLarge Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on the LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and $k$-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results demonstrate that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves $26 \\%$ relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around $2 \\%$ higher overall score than the base Mem0 configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to the full-context approach. In particular, Mem0 attains a $91 \\%$ lower p95 latency and saves more than $90 \\%$ token cost, thereby offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight the critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.\n\n\nCode can be found at: https://mem0.ai/research\n\n## 1. Introduction\n\nHuman memory is a foundation of intelligence-it shapes our identity, guides decision-making, and enables us to learn, adapt, and form meaningful relationships (Craik and Jennings, 1992). Among its many roles, memory is essential for communication: we recall past interactions, infer preferences, and construct evolving mental models of those we engage with (Assmann, 2011). This ability to retain and retrieve information over extended periods enables coherent, contextually rich exchanges that span days, weeks, or even months. AI agents, powered by large language models (LLMs), have made remarkable progress in generating fluent, contextually appropriate responses (Yu et al., 2024, Zhang et al., 2024). However, these systems are fundamentally limited by their reliance on fixed context windows, which severely restrict their ability to maintain coherence over extended interactions (Bulatov et al., 2022, Liu et al., 2023). This limitation stems from LLMs' lack of persistent memory mechanisms that can extend beyond their finite context windows. While humans naturally accumulate and organize experiences over time, forming a continuous narrative\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Illustration of memory importance in AI agents. Left: Without persistent memory, the system forgets critical user information (vegetarian, dairy-free preferences) between sessions, resulting in inappropriate recommendations. Right: With effective memory, the system maintains these dietary preferences across interactions, enabling contextually appropriate suggestions that align with previously established constraints.\nof interactions, AI systems cannot inherently persist information across separate sessions or after context overflow. The absence of persistent memory creates a fundamental disconnect in human-AI interaction. Without memory, AI agents forget user preferences, repeat questions, and contradict previously established facts. Consider a simple example illustrated in Figure 1, where a user mentions being vegetarian and avoiding dairy products in an initial conversation. In a subsequent session, when the user asks about dinner recommendations, a system without persistent memory might suggest chicken, completely contradicting the established dietary preferences. In contrast, a system with persistent memory would maintain this critical user information across sessions and suggest appropriate vegetarian, dairy-free options. This common scenario highlights how memory failures can fundamentally undermine user experience and trust.\n\nBeyond conversational settings, memory mechanisms have been shown to dramatically enhance agent performance in interactive environments (Majumder et al., Shinn et al., 2023). Agents equipped with memory of past experiences can better anticipate user needs, learn from previous mistakes, and generalize knowledge across tasks (Chhikara et al., 2023). Research demonstrates that memory-augmented agents improve decision-making by leveraging causal relationships between actions and outcomes, leading to more effective adaptation in dynamic scenarios (Rasmussen et al., 2025). Hierarchical memory architectures (Packer et al., 2023, Sarthi et al., 2024) and agentic memory systems capable of autonomous evolution (Xu et al., 2025) have further shown that memory enables more coherent, long-term reasoning across multiple dialogue sessions.\n\nUnlike humans, who dynamically integrate new information and revise outdated beliefs, LLMs effectively \"reset\" once information falls outside their context window (Zhang, 2024, Timoneda and Vera, 2025). Even as models like OpenAI's GPT-4 (128K tokens) (Hurst et al., 2024), o1 (200K context) (Jaech et al., 2024), Anthropic's Claude 3.7 Sonnet (200K tokens) (Anthropic, 2025), and Google's Gemini (at least 10M tokens) (Team et al., 2024) push the boundaries of context length, these improvements merely delay rather than solve the fundamental limitation. In practical applications, even these extended context windows prove insufficient for two critical reasons. First, as meaningful human-AI relationships develop over weeks or months, conversation history inevitably exceeds even the most generous context limits. Second, and perhaps more importantly, real-world conversations rarely maintain thematic continuity. A user might mention dietary preferences (being vegetarian), then engage in hours of unrelated discussion about programming tasks, before returning to food-related queries about dinner options. In such scenarios, a full-context approach would need to reason through mountains of irrelevant information, with the critical dietary preferences potentially buried among thousands of tokens of coding discussions. Moreover, simply presenting longer contexts does not ensure effective retrieval or utilization of past information, as attention mechanisms\n\ndegrade over distant tokens (Guo et al., 2024, Nelson et al., 2024). This limitation is particularly problematic in high-stakes domains such as healthcare, education, and enterprise support, where maintaining continuity and trust is crucial (Hatalis et al., 2023). To address these challenges, AI agents must adopt memory systems that go beyond static context extension. A robust AI memory should selectively store important information, consolidate related concepts, and retrieve relevant details when needed-mirroring human cognitive processes (He et al., 2024). By integrating such mechanisms, we can develop AI agents that maintain consistent personas, track evolving user preferences, and build upon prior exchanges. This shift will transform AI from transient, forgetful responders into reliable, long-term collaborators, fundamentally redefining the future of conversational intelligence.\n\nIn this paper, we address a fundamental limitation in AI systems: their inability to maintain coherent reasoning across extended conversations across different sessions, which severely restricts meaningful long-term interactions with users. We introduce MemO (pronounced as mem-zero), a novel memory architecture that dynamically captures, organizes, and retrieves salient information from ongoing conversations. Building on this foundation, we develop Memo ${ }^{\\circledR}$, which enhances the base architecture with graph-based memory representations to better model complex relationships between conversational elements. Our experimental results on the LOCOMO benchmark demonstrate that our approaches consistently outperform existing memory systems-including memory-augmented architectures, retrieval-augmented generation (RAG) methods, and both open-source and proprietary solutions-across diverse question types, while simultaneously requiring significantly lower computational resources. Latency measurements further reveal that MemO operates with $91 \\%$ lower response times than full-context approaches, striking an optimal balance between sophisticated reasoning capabilities and practical deployment constraints. These contributions represent a meaningful step toward AI systems that can maintain coherent, context-aware conversations over extended durations-mirroring human communication patterns and opening new possibilities for applications in personal tutoring, healthcare, and personalized assistance.\n\n# 2. Proposed Methods \n\nWe introduce two memory architectures for AI agents. (1) MemO implements a novel paradigm that extracts, evaluates, and manages salient information from conversations through dedicated modules for memory extraction and updation. The system processes a pair of messages between either two user participants or a user and an assistant. (2) Memo ${ }^{\\circledR}$ extends this foundation by incorporating graph-based memory representations, where memories are stored as directed labeled graphs with entities as nodes and relationships as edges. This structure enables a deeper understanding of the connections between entities. By explicitly modeling both entities and their relationships, Memo ${ }^{\\circledR}$ supports more advanced reasoning across interconnected facts, especially for queries that require navigating complex relational paths across multiple memories.\n\n### 2.1. MemO\n\nOur architecture follows an incremental processing paradigm, enabling it to operate seamlessly within ongoing conversations. As illustrated in Figure 2, the complete pipeline architecture consists of two phases: extraction and update.\n\nThe extraction phase initiates upon ingestion of a new message pair $\\left(m_{t-1}, m_{t}\\right)$, where $m_{t}$ represents the current message and $m_{t-1}$ the preceding one. This pair typically consists of a user message and an assistant response, capturing a complete interaction unit. To establish appropriate context for memory extraction, the system employs two complementary sources: (1) a conversation summary $S$ retrieved from the database that\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Architectural overview of the MemO system showing extraction and update phase. The extraction phase processes messages and historical context to create new memories. The update phase evaluates these extracted memories against similar existing ones, applying appropriate operations through a Tool Call mechanism. The database serves as the central repository, providing context for processing and storing updated memories.\nencapsulates the semantic content of the entire conversation history, and (2) a sequence of recent messages $\\left\\{m_{t-m}, m_{t-m+1}, \\ldots, m_{t-2}\\right\\}$ from the conversation history, where $m$ is a hyperparameter controlling the recency window. To support context-aware memory extraction, we implement an asynchronous summary generation module that periodically refreshes the conversation summary. This component operates independently of the main processing pipeline, ensuring that memory extraction consistently benefits from up-to-date contextual information without introducing processing delays. While $S$ provides global thematic understanding across the entire conversation, the recent message sequence offers granular temporal context that may contain relevant details not consolidated in the summary. This dual contextual information, combined with the new message pair, forms a comprehensive prompt $P=\\left(S,\\left\\{m_{t-m}, \\ldots, m_{t-2}\\right\\}, m_{t-1}, m_{t}\\right)$ for an extraction function $\\phi$ implemented via an LLM. The function $\\phi(P)$ then extracts a set of salient memories $\\Omega=\\left\\{\\omega_{1}, \\omega_{2}, \\ldots, \\omega_{n}\\right\\}$ specifically from the new exchange while maintaining awareness of the conversation's broader context, resulting in candidate facts for potential inclusion in the knowledge base.\n\nFollowing extraction, the update phase evaluates each candidate fact against existing memories to maintain consistency and avoid redundancy. This phase determines the appropriate memory management operation for each extracted fact $\\omega_{i} \\in \\Omega$. Algorithm 1, mentioned in Appendix B, illustrates this process. For each fact, the system first retrieves the top $s$ semantically similar memories using vector embeddings from the database. These retrieved memories, along with the candidate fact, are then presented to the LLM through a function-calling interface we refer to as a 'tool call.' The LLM itself determines which of four distinct operations to execute: ADD for creation of new memories when no semantically equivalent memory exists; UPDATE for augmentation of existing memories with complementary information; DELETE for removal of memories contradicted by new information; and NOOP when the candidate fact requires no modification to the knowledge base. Rather than using a separate classifier, we leverage the LLM's reasoning capabilities to directly select the appropriate operation based on the semantic relationship between the candidate fact and existing memories. Following this determination, the system executes the provided operations, thereby maintaining knowledge base coherence and temporal consistency.\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: Graph-based memory architecture of $\\mathrm{MemO}^{g}$ illustrating entity extraction and update phase. The extraction phase uses LLMs to convert conversation messages into entities and relation triplets. The update phase employs conflict detection and resolution mechanisms when integrating new information into the existing knowledge graph.\n\nIn our experimental evaluation, we configured the system with ' $m$ ' $=10$ previous messages for contextual reference and ' $s$ ' $=10$ similar memories for comparative analysis. All language model operations utilized GPT-40-mini as the inference engine. The vector database employs dense embeddings to facilitate efficient similarity search during the update phase.\n\n# 2.2. $\\mathrm{MemO}^{g}$ \n\nThe $\\mathrm{MemO}^{g}$ pipeline, illustrated in Figure 3, implements a graph-based memory approach that effectively captures, stores, and retrieves contextual information from natural language interactions (Zhang et al., 2022). In this framework, memories are represented as a directed labeled graph $G=(V, E, L)$, where:\n\n- Nodes $V$ represent entities (e.g., Alice, San_Francisco)\n- Edges $E$ represent relationships between entities (e.g., LIVES_IN)\n- Labels $L$ assign semantic types to nodes (e.g., Alice - Person, San_Francisco - City)\n\nEach entity node $v \\in V$ contains three components: (1) an entity type classification that categorizes the entity (e.g., Person, Location, Event), (2) an embedding vector $e_{v}$ that captures the entity's semantic meaning, and (3) metadata including a creation timestamp $t_{v}$. Relationships in our system are structured as triplets in the form $\\left(v_{s}, r, v_{d}\\right)$, where $v_{s}$ and $v_{d}$ are source and destination entity nodes, respectively, and $r$ is the labeled edge connecting them.\n\nThe extraction process employs a two-stage pipeline leveraging LLMs to transform unstructured text into structured graph representations. First, an entity extractor module processes the input text to identify a set of entities along with their corresponding types. In our framework, entities represent the key information elements in conversations-including people, locations, objects, concepts, events, and attributes that merit representation in the memory graph. The entity extractor identifies these diverse information units by analyzing the semantic importance, uniqueness, and persistence of elements in the conversation. For instance, in a conversation about travel plans, entities might include destinations (cities, countries), transportation modes, dates, activities, and participant preferences-essentially any discrete information that could be relevant for future reference or reasoning.\n\nNext, a relationship generator component derives meaningful connections between these entities, establishing a set of relationship triplets that capture the semantic structure of the information. This LLMbased module analyzes the extracted entities and their context within the conversation to identify semantically significant connections. It works by examining linguistic patterns, contextual cues, and domain knowledge to determine how entities relate to one another. For each potential entity pair, the generator evaluates whether a meaningful relationship exists and, if so, classifies this relationship with an appropriate label (e.g., 'lives_in', 'prefers', 'owns', 'happened_on'). The module employs prompt engineering techniques that guide the LLM to reason about both explicit statements and implicit information in the dialogue, resulting in relationship triplets that form the edges in our memory graph and enable complex reasoning across interconnected information. When integrating new information, Memo ${ }^{\\circledR}$ employs a sophisticated storage and update strategy. For each new relationship triple, we compute embeddings for both source and destination entities, then search for existing nodes with semantic similarity above a defined threshold ' $t$ '. Based on node existence, the system may create both nodes, create only one node, or use existing nodes before establishing the relationship with appropriate metadata. To maintain a consistent knowledge graph, we implement a conflict detection mechanism that identifies potentially conflicting existing relationships when new information arrives. An LLM-based update resolver determines if certain relationships should be obsolete, marking them as invalid rather than physically removing them to enable temporal reasoning.\n\nThe memory retrieval functionality in Memo ${ }^{\\circledR}$ implements a dual-approach strategy for optimal information access. The entity-centric method first identifies key entities within a query, then leverages semantic similarity to locate corresponding nodes in the knowledge graph. It systematically explores both incoming and outgoing relationships from these anchor nodes, constructing a comprehensive subgraph that captures relevant contextual information. Complementing this, the semantic triplet approach takes a more holistic view by encoding the entire query as a dense embedding vector. This query representation is then matched against textual encodings of each relationship triplet in the knowledge graph. The system calculates fine-grained similarity scores between the query and all available triplets, returning only those that exceed a configurable relevance threshold, ranked in order of decreasing similarity. This dual retrieval mechanism enables Memo ${ }^{\\circledR}$ to handle both targeted entity-focused questions and broader conceptual queries with equal effectiveness.\n\nFrom an implementation perspective, the system utilizes Neo $4 \\mathrm{j}^{1}$ as the underlying graph database. LLMbased extractors and update module leverage GPT-4o-mini with function calling capabilities, allowing for structured extraction of information from unstructured text. By combining graph-based representations with semantic embeddings and LLM-based information extraction, Memo ${ }^{\\circledR}$ achieves both the structural richness needed for complex reasoning and the semantic flexibility required for natural language understanding.\n\n# 3. Experimental Setup \n\n### 3.1. Dataset\n\nThe LOCOMO (Maharana et al., 2024) dataset is designed to evaluate long-term conversational memory in dialogue systems. It comprises 10 extended conversations, each containing approximately 600 dialogues and 26000 tokens on average, distributed across multiple sessions. Each conversation captures two individuals discussing daily experiences or past events. Following these multi-session dialogues, each conversation is accompanied by 200 questions on an average with corresponding ground truth answers. These questions are categorized into multiple types: single-hop, multi-hop, temporal, and open-domain. The dataset originally included an adversarial question category, which was designed to test systems' ability to recognize\n\n[^0]\n[^0]:    ${ }^{1}$ https://neo4j.com/\n\nunanswerable questions. However, this category was excluded from our evaluation because ground truth answers were unavailable, and the expected behavior for this question type is that the agent should recognize them as unanswerable.\n\n# 3.2. Evaluation Metrics \n\nOur evaluation framework implements a comprehensive approach to assess long-term memory capabilities in dialogue systems, considering both response quality and operational efficiency. We categorize our metrics into two distinct groups that together provide a holistic understanding of system performance.\n(1) Performance Metrics Previous research in conversational AI (Goswami, 2025, Soni et al., 2024, Singh et al., 2020) has predominantly relied on lexical similarity metrics such as F1 Score ( $\\mathrm{F}_{1}$ ) and BLEU-1 ( $\\mathrm{B}_{1}$ ). However, these metrics exhibit significant limitations when evaluating factual accuracy in conversational contexts. Consider a scenario where the ground truth answer is 'Alice was born in March' and a system generates 'Alice is born in July.' Despite containing a critical factual error regarding the birth month, traditional metrics would assign relatively high scores due to lexical overlap in the remaining tokens ('Alice,' 'born,' etc.). This fundamental limitation can lead to misleading evaluations that fail to capture semantic correctness. To address these shortcomings, we use LLM-as-a-Judge (J) as a complementary evaluation metric. This approach leverages a separate, more capable LLM to assess response quality across multiple dimensions, including factual accuracy, relevance, completeness, and contextual appropriateness. The judge model analyzes the question, ground truth answer and the generated answer, providing a more nuanced evaluation that aligns better with human judgment. Due to the stochastic nature of J evaluations, we conducted 10 independent runs for each method on the entire dataset and report the mean scores along with $\\pm 1$ standard deviation. More details about the J is present in Appendix A.\n(2) Deployment Metrics Beyond response quality, practical deployment considerations are crucial for realworld applications of long-term memory in AI agents. We systematically track Token Consumption, using 'cl100k_base' encoding from tiktoken, measuring the number of tokens extracted during retrieval that serve as context for answering queries. For our memory-based models, these tokens represent the memories retrieved from the knowledge base, while for RAG-based models, they correspond to the total number of tokens in the retrieved text chunks. This distinction is important as it directly affects operational costs and system efficiency-whether processing concise memory facts or larger raw text segments. We further monitor Latency, (i) search latency: which captures the total time required to search the memory (in memory-based solutions) or chunk (in RAG-based solutions) and (ii) total latency: time to generate appropriate responses, consisting of both retrieval time (accessing memories or chunks) and answer generation time using the LLM.\n\nThe relationship between these metrics reveals important trade-offs in system design. For instance, more sophisticated memory architectures might achieve higher factual accuracy but at the cost of increased token consumption and latency. Our multi-dimensional evaluation methodology enables researchers and practitioners to make informed decisions based on their specific requirements, whether prioritizing response quality for critical applications or computational efficiency for real-time deployment scenarios.\n\n### 3.3. Baselines\n\nTo comprehensively evaluate our approach, we compare against six distinct categories of baselines that represent the current state of conversational memory systems. These diverse baselines collectively provide a\n\nrobust framework for evaluating the effectiveness of different memory architectures across various dimensions, including factual accuracy, computational efficiency, and scalability to extended conversations. Where applicable, unless otherwise specified, we set the temperature to 0 to ensure the runs are as reproducible as possible.\n\nEstablished LOCOMO Benchmarks We first establish a comparative foundation by evaluating previously benchmarked methods on the LOCOMO dataset. These include five established approaches: LoCoMo (Maharana et al., 2024), ReadAgent (Lee et al., 2024), MemoryBank (Zhong et al., 2024), MemGPT (Packer et al., 2023), and A-Mem (Xu et al., 2025). These established benchmarks not only provide direct comparison points with published results but also represent the evolution of conversational memory architectures across different algorithmic paradigms. For our evaluation, we select the metrics where gpt-4o-mini was used for the evaluation. More details about these benchmarks are mentioned in Appendix C.\n\nOpen-Source Memory Solutions Our second category consists of promising open-source memory architectures such as LangMem ${ }^{2}$ (Hot Path) that have demonstrated effectiveness in related conversational tasks but have not yet been evaluated on the LOCOMO dataset. By adapting these systems to our evaluation framework, we broaden the comparative landscape and identify potential alternative approaches that may offer competitive performance. We initialized the LLM with gpt-4o-mini and used text-embedding-small-3 as the embedding model.\n\nRetrieval-Augmented Generation (RAG) As a baseline, we treat the entire conversation history as a document collection and apply a standard RAG pipeline. We first segment each conversation into fixed-length chunks (128, 256, 512, 1024, 2048, 4096, and 8192 tokens), where 8192 is the maximum chunk size supported by our embedding model. All chunks are embedded using OpenAI's text-embedding-small-3 to ensure consistent vector quality across configurations. At query time, we retrieve the top $k$ chunks by semantic similarity and concatenate them as context for answer generation. Throughout our experiments we set $k \\in\\{1,2\\}$ : with $k=1$ only the single most relevant chunk is used, and with $k=2$ the two most relevant chunks (up to 16384 tokens) are concatenated. We avoid $k>2$ since the average conversation length (26000 tokens) would be fully covered, negating the benefits of selective retrieval. By varying chunk size and $k$, we systematically evaluate RAG performance on long-term conversational memory tasks.\n\nFull-Context Processing We adopt a straightforward approach by passing the entire conversation history within the context window of the LLM. This method leverages the model's inherent ability to process sequential information without additional architectural components. While conceptually simple, this approach faces practical limitations as conversation length increases, eventually increasing token cost and latency. Nevertheless, it establishes an important reference point for understanding the value of more sophisticated memory mechanisms compared to direct processing of available context.\n\nProprietary Models We evaluate OpenAI's memory ${ }^{3}$ feature available in their ChatGPT interface, specifically using gpt-4o-mini for consistency. We ingest entire LOCOMO conversations with a prompt (see Appendix A) into single chat sessions, prompting memory generation with timestamps, participant names, and conversation\n\n[^0]\n[^0]:    ${ }^{2}$ https://langchain-ai.github.io/langmem/\n    ${ }^{3}$ https://openai.com/index/memory-and-new-controls-for-chatgpt/\n\nTable 1: Performance comparison of memory-enabled systems across different question types in the LOCOMO dataset. Evaluation metrics include F1 score $\\left(\\mathrm{F}_{1}\\right)$, BLEU-1 $\\left(\\mathrm{B}_{1}\\right)$, and LLM-as-a-Judge score (J), with higher values indicating better performance. A-Mem ${ }^{\\star}$ represents results from our re-run of A-Mem to generate LLM-as-a-Judge scores by setting temperature as $0 . \\mathrm{Mem}^{2}$ indicates our proposed architecture enhanced with graph memory. Bold denotes the best performance for each metric across all methods. ( $\\uparrow$ ) represents higher score is better.\n\n| Method | Single Hop |  |  | Multi-Hop |  |  | Open Domain |  |  | Temporal |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | $\\mathrm{F}_{1} \\uparrow$ | $\\mathrm{B}_{1} \\uparrow$ | $\\mathrm{J} \\uparrow$ | $\\mathrm{F}_{1} \\uparrow$ | $\\mathrm{B}_{1} \\uparrow$ | $\\mathrm{J} \\uparrow$ | $\\mathrm{F}_{1} \\uparrow$ | $\\mathrm{B}_{1} \\uparrow$ | $\\mathrm{J} \\uparrow$ | $\\mathrm{F}_{1} \\uparrow$ | $\\mathrm{B}_{1} \\uparrow$ | $\\mathrm{J} \\uparrow$ |\n| LoCoMo | 25.02 | 19.75 | - | 12.04 | 11.16 | - | 40.36 | 29.05 | - | 18.41 | 14.77 | - |\n| ReadAgent | 9.15 | 6.48 | - | 5.31 | 5.12 | - | 9.67 | 7.66 | - | 12.60 | 8.87 | - |\n| MemoryBank | 5.00 | 4.77 | - | 5.56 | 5.94 | - | 6.61 | 5.16 | - | 9.68 | 6.99 | - |\n| MemGPT | 26.65 | 17.72 | - | 9.15 | 7.44 | - | 41.04 | 34.34 | - | 25.52 | 19.44 | - |\n| A-Mem | 27.02 | 20.09 | - | 12.14 | 12.00 | - | 44.65 | 37.06 | - | 45.85 | 36.67 | - |\n| A-Mem* | 20.76 | 14.90 | $39.79 \\pm 0.38$ | 9.22 | 8.81 | $18.85 \\pm 0.31$ | 33.34 | 27.58 | $54.05 \\pm 0.22$ | 35.40 | 31.08 | $49.91 \\pm 0.31$ |\n| LangMem | 35.51 | 26.86 | $62.23 \\pm 0.75$ | 26.04 | 22.32 | $47.92 \\pm 0.47$ | 40.91 | 33.63 | $71.12 \\pm 0.20$ | 30.75 | 25.84 | $23.43 \\pm 0.39$ |\n| Zep | 35.74 | 23.30 | $61.70 \\pm 0.32$ | 19.37 | 14.82 | $41.35 \\pm 0.48$ | 49.56 | 38.92 | $76.60 \\pm 0.13$ | 42.00 | 34.53 | $49.31 \\pm 0.50$ |\n| OpenAI | 34.30 | 23.72 | $63.79 \\pm 0.46$ | 20.09 | 15.42 | $42.92 \\pm 0.63$ | 39.31 | 31.16 | $62.29 \\pm 0.12$ | 14.04 | 11.25 | $21.71 \\pm 0.20$ |\n| MemO | 38.72 | 27.13 | $67.13 \\pm 0.65$ | 28.64 | 21.58 | $51.15 \\pm 0.31$ | 47.65 | 38.72 | $72.93 \\pm 0.11$ | 48.93 | 40.51 | $55.51 \\pm 0.34$ |\n| Memo ${ }^{2}$ | 38.09 | 26.03 | $65.71 \\pm 0.45$ | 24.32 | 18.82 | $47.19 \\pm 0.67$ | 49.27 | 40.30 | $75.71 \\pm 0.21$ | 51.55 | 40.28 | $58.13 \\pm 0.44$ |\n\ntext. These generated memories are then used as complete context for answering questions about each conversation, intentionally granting the OpenAI approach privileged access to all memories rather than only question-relevant ones. This methodology accommodates the lack of external API access for selective memory retrieval in OpenAI's system for benchmarking.\n\nMemory Providers We incorporate Zep (Rasmussen et al., 2025), a memory management platform designed for AI agents. Using their platform version, we conduct systematic evaluations across the LOCOMO dataset, maintaining temporal fidelity by preserving timestamp information alongside conversational content. This temporal anchoring ensures that time-sensitive queries can be addressed through appropriately contextualized memory retrieval, particularly important for evaluating questions that require chronological awareness. This baseline represents an important commercial implementation of memory management specifically engineered for AI agents.\n\n# 4. Evaluation Results, Analysis and Discussion. \n\n### 4.1. Performance Comparison Across Memory-Enabled Systems\n\nTable 1 reports $\\mathrm{F}_{1}, \\mathrm{~B}_{1}$ and J scores for our two architectures-Mem0 and Mem0 ${ }^{2}$ —against a suite of competitive baselines, as mentioned in Section 3, on single-hop, multi-hop, open-domain, and temporal questions. Overall, both of our models set new state-of-the-art marks in all the three evaluation metrics for most question types.\n\nSingle-Hop Question Performance Single-hop queries involve locating a single factual span contained within one dialogue turn. Leveraging its dense memories in natural language text, Mem0 secures the strongest results: $\\mathrm{F}_{1}=38.72, \\mathrm{~B}_{1}=27.13$, and $\\mathrm{J}=67.13$. Augmenting the natural language memories with graph memory (Mem0 ${ }^{2}$ ) yields marginal performance drop compared to Mem0, indicating that relational structure provides limited utility when the retrieval target occupies a single turn. Among the existing baselines, the full-context OpenAI run attains the next-best J score, reflecting the benefits of retaining the entire conversation in context, while LangMem and Zep both score around 8\\% relatively less against our models on J score. Previous LOCOMO\n\nbenchmarks such as A-mem lag by more than 25 points in J, underscoring the necessity of fine-grained, structured memory indexing even for simple retrieval tasks.\n\nMulti-Hop Question Performance Multi-hop queries require synthesizing information dispersed across multiple conversation sessions, posing significant challenges in memory integration and retrieval. MemO clearly outperforms other methods with an $\\mathrm{F}_{1}$ score of 28.64 and a J score of 51.15 , reflecting its capability to efficiently retrieve and integrate disparate information stored across sessions. Interestingly, the addition of graph memory in Memo ${ }^{\\mathbb{Z}}$ does not provide performance gains here, indicating potential inefficiencies or redundancies in structured graph representations for complex integrative tasks compared to dense natural language memory alone. Baselines like LangMem show competitive performances, but their scores substantially trail those of MemO, emphasizing the advantage of our refined memory indexing and retrieval mechanisms for complex query processing.\n\nOpen-Domain Performance In open-domain settings, the baseline Zep achieves the highest $\\mathrm{F}_{1}$ (49.56) and J (76.60) scores, edging out our methods by a narrow margin. In particular, Zep's J score of 76.60 surpasses Memo ${ }^{\\mathbb{Z}}$ 's 75.71 by just 0.89 percentage points and outperforms MemO's 72.93 by 3.67 points, highlighting a consistent, if slight, advantage in integrating conversational memory with external knowledge. Memo ${ }^{\\mathbb{Z}}$ remains a strong runner-up, with a J of 75.71 reflecting high factual retrieval precision, while MemO follows with 72.93, demonstrating robust coherence. These results underscore that although structured relational memories (as in MemO and Memo ${ }^{\\mathbb{Z}}$ ) substantially improve open-domain retrieval, Zep maintains a small but meaningful lead.\n\nTemporal Reasoning Performance Temporal reasoning tasks hinge on accurate modeling of event sequences, their relative ordering, and durations within conversational history. Our architectures demonstrate substantial improvements across all metrics, with Memo ${ }^{\\mathbb{Z}}$ achieving the highest $\\mathrm{F}_{1}(51.55)$ and J (58.13), suggesting that structured relational representations in addition to natural language memories significantly aid in temporally grounded judgments. Notably, the base variant, MemO, also provide a decent J score (55.51), suggesting that natural language alone can aid in temporally grounded judgments. Among baselines, OpenAI notably underperforms, with scores below $15 \\%$, primarily due to missing timestamps in most generated memories despite explicit prompting in the OpenAI ChatGPT to extract memories with timestamps. Other baselines such as A-Mem achieve respectable results, yet our models clearly advance the state-of-the-art, emphasizing the critical advantage of accurately leveraging both natural language contextualization and structured graph representations for temporal reasoning.\n\n# 4.2. Cross-Category Analysis \n\nThe comprehensive evaluation across diverse question categories reveals that our proposed architectures, MemO and Memo ${ }^{\\mathbb{Z}}$, consistently achieve superior performance compared to baseline systems. For single-hop queries, MemO demonstrates particularly strong performance, benefiting from its efficient dense natural language memory structure. Although graph-based representations in Memo ${ }^{\\mathbb{Z}}$ slightly lag behind in lexical overlap metrics for these simpler queries, they significantly enhance semantic coherence, as demonstrated by competitive J scores. This indicates that graph structures are more beneficial in scenarios involving nuanced relational context rather than straightforward retrieval. For multi-hop questions, MemO exhibits clear advantages by effectively synthesizing dispersed information across multiple sessions, confirming that natural language memories provide sufficient representational richness for these integrative tasks. Surprisingly, the\n\nTable 2: Performance comparison of various baselines with proposed methods. Latency measurements show p50 (median) and p95 (95th percentile) values in seconds for both search time (time taken to fetch memories/chunks) and total time (time to generate the complete response). Overall LLM-as-a-Judge score (J) represents the quality metric of the generated responses on the entire LOCOMO dataset.\n\n| Method |  |  | Latency (seconds) |  |  |  | Overall J |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  |  | Search |  | Total |  |  |\n|  | K | chunk size / memory tokens | p50 | p95 | p50 | p95 |  |\n| RAG | 1 | 128 | 0.281 | 0.823 | 0.774 | 1.825 | $47.77 \\pm 0.23 \\%$ |\n|  |  | 256 | 0.251 | 0.710 | 0.745 | 1.628 | $50.15 \\pm 0.16 \\%$ |\n|  |  | 512 | 0.240 | 0.639 | 0.772 | 1.710 | $46.05 \\pm 0.14 \\%$ |\n|  | 1 | 1024 | 0.240 | 0.723 | 0.821 | 1.957 | $40.74 \\pm 0.17 \\%$ |\n|  |  | 2048 | 0.255 | 0.752 | 0.996 | 2.182 | $37.93 \\pm 0.12 \\%$ |\n|  |  | 4096 | 0.254 | 0.719 | 1.093 | 2.711 | $36.84 \\pm 0.17 \\%$ |\n|  |  | 8192 | 0.279 | 0.838 | 1.396 | 4.416 | $44.53 \\pm 0.13 \\%$ |\n|  | 2 | 128 | 0.267 | 0.624 | 0.766 | 1.829 | $59.56 \\pm 0.19 \\%$ |\n|  |  | 256 | 0.255 | 0.699 | 0.802 | 1.907 | $60.97 \\pm 0.20 \\%$ |\n|  |  | 512 | 0.247 | 0.746 | 0.829 | 1.729 | $58.19 \\pm 0.18 \\%$ |\n|  |  | 1024 | 0.238 | 0.702 | 0.860 | 1.850 | $50.68 \\pm 0.13 \\%$ |\n|  |  | 2048 | 0.261 | 0.829 | 1.101 | 2.791 | $48.57 \\pm 0.22 \\%$ |\n|  |  | 4096 | 0.266 | 0.944 | 1.451 | 4.822 | $51.79 \\pm 0.15 \\%$ |\n|  |  | 8192 | 0.288 | 1.124 | 2.312 | 9.942 | $60.53 \\pm 0.16 \\%$ |\n| Full-context |  | 26031 | - | - | 9.870 | 17.117 | $72.90 \\pm 0.19 \\%$ |\n| A-Mem |  | 2520 | 0.668 | 1.485 | 1.410 | 4.374 | $48.38 \\pm 0.15 \\%$ |\n| LangMem |  | 127 | 17.99 | 59.82 | 18.53 | 60.40 | $58.10 \\pm 0.21 \\%$ |\n| Zep |  | 3911 | 0.513 | 0.778 | 1.292 | 2.926 | $65.99 \\pm 0.16 \\%$ |\n| OpenAI |  | 4437 | - | - | 0.466 | 0.889 | $52.90 \\pm 0.14 \\%$ |\n| MemO |  | 1764 | 0.148 | 0.200 | 0.708 | 1.440 | $66.88 \\pm 0.15 \\%$ |\n| Memo ${ }^{g}$ |  | 3616 | 0.476 | 0.657 | 1.091 | 2.590 | $68.44 \\pm 0.17 \\%$ |\n\nexpected relational advantages of $\\mathrm{MemO}^{g}$ do not translate into better outcomes here, suggesting potential overhead or redundancy when navigating more intricate graph structures in multi-step reasoning scenarios.\n\nIn temporal reasoning, Memo ${ }^{g}$ substantially outperforms other methods, validating that structured relational graphs excel in capturing chronological relationships and event sequences. The presence of explicit relational context significantly enhances Memo ${ }^{g}$ 's temporal coherence, outperforming MemO's dense memory storage and highlighting the importance of precise relational representations when tracking temporally sensitive information. Open-domain performance further reinforces the value of relational modeling. Memo ${ }^{g}$, benefiting from the relational clarity of graph-based memory, closely competes with the top-performing baseline (Zep). This competitive result underscores Memo ${ }^{g}$ 's robustness in integrating external knowledge through relational clarity, suggesting an optimal synergy between structured memory and open-domain information synthesis.\n\n![img-3.jpeg](img-3.jpeg)\n(a) Comparison of search latency at p50 (median) and p95 (95th percentile) across different memory methods (Mem0, Mem0 ${ }^{\\mathbb{Z}}$, best RAG variant, Zep, LangMem, and A-Mem). The bar heights represent J scores (left axis), while the line plots show search latency in seconds (right axis scaled in log).\n![img-4.jpeg](img-4.jpeg)\n(b) Comparison of total response latency at p50 and p95 across different memory methods (Mem0, Mem0 ${ }^{\\mathbb{Z}}$, best RAG variant, Zep, LangMem, OpenAI, full-context, and A-Mem). The bar heights represent J scores (left axis), and the line plots capture end-to-end latency in seconds (right axis scaled in log).\n\nFigure 4: Latency Analysis of Different Memory Approaches. These subfigures illustrate the J scores and latency comparison of various selected methods from Table 2. Subfigure (a) highlights the search/retrieval latency prior to answer generation, while Subfigure (b) shows the total latency (including LLM inference). Both plots overlay each method's J score for a holistic view of their accuracy and efficiency.\n\nOverall, our analysis indicates complementary strengths of Mem0 and Mem0 ${ }^{\\mathbb{Z}}$ across various task demands: dense, natural-language-based memory offers significant efficiency for simpler queries, while explicit relational modeling becomes essential for tasks demanding nuanced temporal and contextual integration. These findings reinforce the importance of adaptable memory structures tailored to specific reasoning contexts in AI agent deployments.\n\n# 4.3. Performance Comparison of Mem0 and Mem0 ${ }^{\\text { }}$ Against RAG Approaches and Full-Context Model \n\nComparisons in Table 2, focusing on the 'Overall J' column, reveal that both Mem0 and Mem0 ${ }^{\\text {® }}$ consistently outperform all RAG configurations, which vary chunk sizes (128-8192 tokens) and retrieve either one $(k=1)$ or two $(k=2)$ chunks. Even the strongest RAG approach peaks at around $61 \\%$ in the J metric, whereas Mem0 reaches $67 \\%$-about a $10 \\%$ relative improvement-and Mem0 ${ }^{\\text {® }}$ reaches over $68 \\%$, achieving around a $12 \\%$ relative gain. These advances underscore the advantage of capturing only the most salient facts in memory, rather than retrieving large chunk of original text. By converting the conversation history into concise, structured representations, Mem0 and Mem0 ${ }^{\\text {® }}$ mitigate noise and surface more precise cues to the LLM, leading to better answers as evaluated by an external LLM (J).\n\nDespite these improvements, a full-context method that ingests a chunk of roughly 26,000 tokens still achieves the highest J score (approximately 73\\%). However, as shown in Figure 4b, it also incurs a very high total p95 latency-around 17 seconds-since the model must read the entire conversation on every query. By contrast, Mem0 and Mem0 ${ }^{\\text {® }}$ significantly reduce token usage and thus achieve lower p95 latencies of around 1.44 seconds (a $92 \\%$ reduction) and 2.6 seconds (a $85 \\%$ reduction), respectively over full-context approach. Although the full-context approach can provide a slight accuracy edge, the memory-based systems offer a more practical trade-off, maintaining near-competitive quality while imposing only a fraction of the token and latency cost. As conversation length increases, full-context approaches suffer from exponential growth in computational overhead (evident in Table 2 where total p95 latency increases significantly with larger $k$ values or chunk sizes). This increase in input chunks leads to longer response times and higher token consumption costs. In contrast, memory-focused approaches like Mem0 and Mem0 ${ }^{\\text {® }}$ maintain consistent performance regardless of conversation length, making them substantially more viable for production-scale deployments where efficiency and responsiveness are critical.\n\n### 4.4. Latency Analysis\n\nTable 2 provides a comprehensive performance comparison of various retrieval and memory methodologies, presenting median (p50) and tail (p95) latencies for both the search phase and total response generation across the LOCOMO dataset. Our analysis reveals distinct performance patterns governed by architectural choices. Memory-centric architectures demonstrate different performance characteristics. A-Mem, despite its larger memory store, incurs substantial search overhead (p50: 0.668 s ), resulting in total median latencies of 1.410s. LangMem exhibits even higher search latencies (p50: 17.99s, p95: 59.82s), rendering it impractical for interactive applications. Zep achieves moderate performance (p50 total: 1.292s). The full-context baseline, which processes the entire conversation history without retrieval, fundamentally differs from retrieval-based approaches. By passing the entire conversation context ( 26000 tokens) directly to the LLM, it eliminates search overhead but incurs extreme total latencies (p50: 9.870s, p95: 17.117s). Similarly, the OpenAI implementation does not perform memory search, as it processes manually extracted memories from their playground. While this approach achieves impressive response generation times (p50: 0.466s, p95: 0.889 s ), it requires pre-extraction of relevant context, which is not reflected in the reported metrics.\n\nOur proposed Mem0 approach achieves the lowest search latency among all methods (p50: 0.148s, p95: 0.200 s ) as illustrated in Figure 4a. This efficiency stems from our selective memory retrieval mechanism and infra improvements that dynamically identifies and retrieves only the most salient information rather than fixed-size chunks. Consequently, Mem0 maintains the lowest total median latency ( 0.708 s ) with remarkably contained p95 values ( 1.440 s ), making it particularly suitable for latency-sensitive applications such as interactive AI agents. The graph-enhanced Mem0 ${ }^{\\text {® }}$ variant introduces additional relational modeling capabilities at a moderate latency cost, with search times ( 0.476 s ) still outperforming all existing memory\n\nsolutions and baselines. Despite this increase, Memo ${ }^{\\circledR}$ maintains competitive total latencies (p50: 1.091s, p95: 2.590s) while achieving the highest J score ( $68.44 \\%$ ) across all methods-trailing only the computationally prohibitive full-context approach. This performance profile demonstrates our methods' ability to balance response quality and computational efficiency, offering a compelling solution for production AI agents where both factors are critical constraints.\n\n# 4.5. Memory System Overhead: Token Analysis and Construction Time \n\nWe measure the average token budget required to materialise each system's long-term memory store. Memo encodes complete dialogue turns in a natural language representation and therefore occupies only $\\mathbf{7 k}$ tokens per conversation on an average. Where as Memo ${ }^{\\circledR}$ roughly doubles the footprint to $\\mathbf{1 4 k}$ tokens, due to the introduction of graph memories which includes nodes and corresponding relationships. In stark contrast, Zep's memory graph consumes in excess of $\\mathbf{6 0 0 k}$ tokens. The inflation arises from Zep's design choice to cache a full abstractive summary at every node while also storing facts on the connecting edges, leading to extensive redundancy across the graph. For perspective, supplying the entire raw conversation context to the language model-without any memory abstraction-amounts to roughly $\\mathbf{2 6 k}$ tokens on average, 20 times less relative to Zep's graph. Beyond token inefficiency, our experiments revealed significant operational bottlenecks with Zep. After adding memories to Zep's system, we observed that immediate memory retrieval attempts often failed to answer our queries correctly. Interestingly, re-running identical searches after a delay of several hours yielded considerably better results. This latency suggests that Zep's graph construction involves multiple asynchronous LLM calls and extensive background processing, making the memory system impractical for real-time applications. In contrast, Memo graph construction completes in under a minute even in worst-case scenarios, allowing users to immediately leverage newly added memories for query responses.\n\nThese findings highlight that Zep not only replicates identical knowledge fragments across multiple nodes, but also introduces significant operational delays. Our architectures-Memo and Memo ${ }^{\\circledR}$ —preserve the same information at a fraction of the token cost and with substantially faster memory availability, offering a more memory-efficient and operationally responsive representation.\n\n## 5. Conclusion and Future Work\n\nWe have introduced Memo and Memo ${ }^{\\circledR}$, two complementary memory architectures that overcome the intrinsic limitations of fixed context windows in LLMs. By dynamically extracting, consolidating, and retrieving compact memory representations, Memo achieves state-of-the-art performance across single-hop and multi-hop reasoning, while Memo ${ }^{\\circledR}$ 's graph-based extensions unlock significant gains in temporal and open-domain tasks. On the LOCOMO benchmark, our methods deliver $5 \\%, 11 \\%$, and $7 \\%$ relative improvements in single-hop, temporal, and multi-hop reasoning question types over best performing methods in respective question type and reduce p95 latency by over $91 \\%$ compared to full-context baselines-demonstrating a powerful balance between precision and responsiveness. Memo's dense memory pipeline excels at rapid retrieval for straightforward queries, minimizing token usage and computational overhead. In contrast, Memo ${ }^{\\circledR}$ 's structured graph representations provide nuanced relational clarity, enabling complex event sequencing and rich context integration without sacrificing practical efficiency. Together, they form a versatile memory toolkit that adapts to diverse conversational demands while remaining deployable at scale.\n\nFuture research directions include optimizing graph operations to reduce the latency overhead in Memo ${ }^{\\circledR}$, exploring hierarchical memory architectures that blend efficiency with relational representation, and developing more sophisticated memory consolidation mechanisms inspired by human cognitive processes.\n\nAdditionally, extending our memory frameworks to domains beyond conversational scenarios, such as procedural reasoning and multimodal interactions, would further validate their broader applicability. By addressing the fundamental limitations of fixed context windows, our work represents a significant advancement toward conversational AI systems capable of maintaining coherent, contextually rich interactions over extended periods, much like their human counterparts.\n\n# 6. Acknowledgments \n\nWe would like to express our sincere gratitude to Harsh Agarwal, Shyamal Anadkat, Prithvijit Chattopadhyay, Siddesh Choudhary, Rishabh Jain, and Vaibhav Pandey for their invaluable insights and thorough reviews of early drafts. Their constructive comments and detailed suggestions helped refine the manuscript, enhancing both its clarity and overall quality. We deeply appreciate their generosity in dedicating time and expertise to this work.\n\n## References\n\nAnthropic. Model card and evaluations for claude models. Technical report, Anthropic, February 2025. URL https://www.anthropic.com/news/claude-3-7-sonnet.\n\nJan Assmann. Communicative and cultural memory. In Cultural memories: The geographical point of view, pages 15-27. Springer, 2011.\n\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079-11091, 2022.\n\nPrateek Chhikara, Jiarui Zhang, Filip Ilievski, Jonathan Francis, and Kaixin Ma. Knowledge-enhanced agents for interactive text games. In Proceedings of the 12th Knowledge Capture Conference 2023, pages 157-165, 2023.\n\nFergus IM Craik and Janine M Jennings. Human memory. 1992.\nGaurav Goswami. Dissecting the metrics: How different evaluation approaches yield diverse results for conversational ai. Authorea Preprints, 2025.\n\nTianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael Jordan, and Song Mei. Active-dormant attention heads: Mechanistically demystifying extreme-token phenomena in llms. In NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning, 2024.\n\nKostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term memory in llm-agents. In Proceedings of the AAAI Symposium Series, volume 2, pages 277-280, 2023.\n\nZihong He, Weizhe Lin, Hao Zheng, Fan Zhang, Matt W Jones, Laurence Aitchison, Xuhai Xu, Miao Liu, Per Ola Kristensson, and Junxiao Shen. Human-inspired perspectives: A survey on ai long-term memory. arXiv preprint arXiv:2411.00489, 2024.\n\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\n\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.\n\nKuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. A human-inspired reading agent with gist memory of very long contexts. In International Conference on Machine Learning, pages 26396-26415. PMLR, 2024.\n\nLei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-inmemory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719, 2023.\n\nAdyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851-13870, 2024.\n\nBodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. Clin: A continually learning language agent for rapid task adaptation and generalization. In First Conference on Language Modeling.\n\nElliot Nelson, Georgios Kollias, Payel Das, Subhajit Chaudhury, and Soham Dan. Needle in the haystack for memory based large language models. arXiv preprint arXiv:2407.01437, 2024.\n\nCharles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. Memgpt: Towards llms as operating systems. 2023.\n\nPreston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. Zep: A temporal knowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956, 2025.\n\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, 2024.\n\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023.\n\nPrabhjot Singh, Prateek Chhikara, and Jasmeet Singh. An ensemble approach for extractive text summarization. In 2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE), pages 1-7. IEEE, 2020.\n\nArpita Soni, Rajeev Arora, Anoop Kumar, and Dheerendra Panwar. Evaluating domain coverage in lowresource generative chatbots: A comparative study of open-domain and closed-domain approaches using bleu scores. In 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT), volume 1, pages 1-6. IEEE, 2024.\n\nGemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.\n\nJoan C Timoneda and Sebastián Vallejo Vera. Memory is all you need: Testing how model memory affects llm performance in annotation tasks. arXiv preprint arXiv:2503.04874, 2025.\n\nWujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025.\n\nYangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced llm trading agent with layered memory and character design. In Proceedings of the AAAI Symposium Series, volume 3, pages 595-597, 2024.\n\nJiarui Zhang. Guided profile generation improves personalization with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 4005-4016, 2024.\n\nJiarui Zhang, Filip Ilievski, Kaixin Ma, Jonathan Francis, and Alessandro Oltramari. A study of zero-shot adaptation with commonsense knowledge. In $A K B C, 2022$.\n\nZeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. A survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024.\n\nWanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19724-19731, 2024.\n\n# Appendix \n\n## A. Prompts\n\nIn developing our LLM-as-a-Judge prompt, we adapt elements from the prompt released by Packer et al. (2023).\n\n## PRoMPT TEMPLATE FOR LLM as a JUDGE\n\nYour task is to label an answer to a question as \"CORRECT\" or \"WRONG\". You will be given the following data: (1) a question (posed by one user to another user), (2) a 'gold' (ground truth) answer, (3) a generated answer which you will score as CORRECT/WRONG.\n\nThe point of the question is to ask about something one user should know about the other user based on their prior conversations. The gold answer will usually be a concise and short answer that includes the referenced topic, for example:\nQuestion: Do you remember what I got the last time I went to Hawaii?\nGold answer: A shell necklace\nThe generated answer might be much longer, but you should be generous with your grading - as long as it touches on the same topic as the gold answer, it should be counted as CORRECT.\n\nFor time related questions, the gold answer will be a specific date, month, year, etc. The generated answer might be much longer or use relative time references (like 'last Tuesday' or 'next month'), but you should be generous with your grading - as long as it refers to the same date or time period as the gold answer, it should be counted as CORRECT. Even if the format differs (e.g., 'May 7th' vs '7 May'), consider it CORRECT if it's the same date.\n\nNow it's time for the real question:\nQuestion: \\{question\\}\nGold answer: \\{gold_answer\\}\nGenerated answer: \\{generated_answer\\}\n\nFirst, provide a short (one sentence) explanation of your reasoning, then finish with CORRECT or WRONG. Do NOT include both CORRECT and WRONG in your response, or it will break the evaluation script.\n\nJust return the label CORRECT or WRONG in a json format with the key as \"label\".\n\n# PROMPT TEMPLATE FOR ReSULtS GENERATION (REN0) \n\nYou are an intelligent memory assistant tasked with retrieving accurate information from conversation memories.\n\n## \\# CONTEXT:\n\nYou have access to memories from two speakers in a conversation. These memories contain timestamped information that may be relevant to answering the question.\n\n## \\# INSTRUCTIONS:\n\n1. Carefully analyze all provided memories from both speakers\n2. Pay special attention to the timestamps to determine the answer\n3. If the question asks about a specific event or fact, look for direct evidence in the memories\n4. If the memories contain contradictory information, prioritize the most recent memory\n5. If there is a question about time references (like \"last year\", \"two months ago\", etc.), calculate the actual date based on the memory timestamp. For example, if a memory from 4 May 2022 mentions \"went to India last year,\" then the trip occurred in 2021.\n6. Always convert relative time references to specific dates, months, or years. For example, convert \"last year\" to \"2022\" or \"two months ago\" to \"March 2023\" based on the memory timestamp. Ignore the reference while answering the question.\n7. Focus only on the content of the memories from both speakers. Do not confuse character names mentioned in memories with the actual users who created those memories.\n8. The answer should be less than 5-6 words.\n\\# APPROACH (Think step by step):\n9. First, examine all memories that contain information related to the question\n10. Examine the timestamps and content of these memories carefully\n11. Look for explicit mentions of dates, times, locations, or events that answer the question\n12. If the answer requires calculation (e.g., converting relative time references), show your work\n13. Formulate a precise, concise answer based solely on the evidence in the memories\n14. Double-check that your answer directly addresses the question asked\n15. Ensure your final answer is specific and avoids vague time references\n\nMemories for user \\{speaker_1_user_id\\}: \\{speaker_1_memories\\}\n\nMemories for user \\{speaker_2_user_id\\}: \\{speaker_2_memories\\}\n\nQuestion: \\{question\\}\n\nAnswer:\n\n# PROMPT TEMPLATE FOR RESULIS GENERATION (RENO) \n\n(same as previous)\n\\# APPROACH (Think step by step):\n\n1. First, examine all memories that contain information related to the question\n2. Examine the timestamps and content of these memories carefully\n3. Look for explicit mentions of dates, times, locations, or events that answer the question\n4. If the answer requires calculation (e.g., converting relative time references), show your work\n5. Analyze the knowledge graph relations to understand the user's knowledge context\n6. Formulate a precise, concise answer based solely on the evidence in the memories\n7. Double-check that your answer directly addresses the question asked\n8. Ensure your final answer is specific and avoids vague time references\n\nMemories for user \\{speaker_1_user_id\\}: \\{speaker_1_memories\\}\n\nRelations for user \\{speaker_1_user_id\\}: \\{speaker_1_graph_memories\\}\n\nMemories for user \\{speaker_2_user_id\\}: \\{speaker_2_memories\\}\n\nRelations for user \\{speaker_2_user_id\\}: \\{speaker_2_graph_memories\\}\n\nQuestion: \\{question\\}\n\nAnswer:\n\n## PROMPT TEMPLATE FOR OPENAI CHATGPT\n\nCan you please extract relevant information from this conversation and create memory entries for each user mentioned? Please store these memories in your knowledge base in addition to the timestamp provided for future reference and personalized interactions.\n(1:56 pm on 8 May, 2023) Caroline: Hey Mel! Good to see you! How have you been? (1:56 pm on 8 May, 2023) Melanie: Hey Caroline! Good to see you! I'm swamped with the kids \\& work. What's up with you? Anything new?\n(1:56 pm on 8 May, 2023) Caroline: I went to a LGBTQ support group yesterday and it was so powerful.\n\n# B. Algorithm \n\n```\nAlgorithm 1 Memory Management System: Update Operations\n    Input: Set of retrieved memories \\(F\\), Existing memory store \\(M=\\left\\{m_{1}, m_{2}, \\ldots, m_{n}\\right\\}\\)\n    Output: Updated memory store \\(M^{\\prime}\\)\n    procedure UpdateMemory \\((F, M)\\)\n        for each fact \\(f \\in F\\) do\n            operation \\(\\leftarrow\\) ClassifyOperation \\((f, M) \\quad \\triangleright\\) Execute appropriate operation based on\n    classification\n            if operation \\(=\\) ADD then\n                id \\(\\leftarrow\\) GenerateUniqueID( )\n                \\(M \\leftarrow M \\cup\\left\\{\\left(i d, f, \" A D D^{\\prime}\\right)\\right\\} \\quad \\triangleright\\) Add new fact with unique identifier\n            else if operation \\(=\\) UPDATE then\n                \\(m_{i} \\leftarrow\\) FindRelatedMemory \\((f, M)\\)\n                if InformationContent \\((f)\\) > InformationContent \\(\\left(m_{i}\\right)\\) then\n                    \\(M \\leftarrow\\left(M \\backslash\\left\\{m_{i}\\right\\}\\right) \\cup\\left\\{\\left(i d_{i}, f, \" U P D A T E^{\\prime}\\right)\\right\\} \\quad \\triangleright\\) Replace with richer information\n                end if\n            else if operation \\(=\\) DELETE then\n                \\(m_{i} \\leftarrow\\) FindContradictedMemory \\((f, M)\\)\n                \\(M \\leftarrow M \\backslash\\left\\{m_{i}\\right\\} \\quad \\triangleright\\) Remove contradicted information\n            else if operation \\(=\\) NOOP then\n                No operation performed \\(\\quad \\triangleright\\) Fact already exists or is irrelevant\n            end if\n        end for\n        return \\(M\\)\n    end procedure\n```\n\nfunction ClassifyOperation $(f, M)$\nif $\\neg$ SemanticallySimilar $(f, M)$ then\nreturn ADD\nelse if Contradicts $(f, M)$ then\nreturn DELETE\nelse if Augments $(f, M)$ then\nreturn UPDATE\nelse\nreturn NOOP\nend if\nend function\n\n## C. Selected Baselines\n\nLoCoMo The LoCoMo framework implements a sophisticated memory pipeline that enables LLM agents to maintain coherent, long-term conversations. At its core, the system divides memory into short-term and long-term components. After each conversation session, agents generate summaries (stored as short-term memory) that distill key information from that interaction. Simultaneously, individual conversation turns are\n\ntransformed into 'observations' - factual statements about each speaker's persona and life events that are stored in long-term memory with references to the specific dialog turns that produced them. When generating new responses, agents leverage both the most recent session summary and selectively retrieve relevant observations from their long-term memory. This dual-memory approach is further enhanced by incorporating a temporal event graph that tracks causally connected life events occurring between conversation sessions. By conditioning responses on retrieved memories, current conversation context, persona information, and intervening life events, the system enables agents to maintain consistent personalities and recall important details across conversations spanning hundreds of turns and dozens of sessions.\n\nReadAgent ReadAgent addresses the fundamental limitations of LLMs by emulating how humans process lengthy texts through a sophisticated three-stage pipeline. First, in Episode Pagination, the system intelligently segments text at natural cognitive boundaries rather than arbitrary cutoffs. Next, during Memory Gisting, it distills each segment into concise summaries that preserve essential meaning while drastically reducing token count-similar to how human memory retains the substance of information without verbatim recall. Finally, when tasked with answering questions, the Interactive Lookup mechanism examines these gists and strategically retrieves only the most relevant original text segments for detailed processing. This human-inspired approach enables LLMs to effectively manage documents up to 20 times longer than their normal context windows. By balancing global understanding through gists with selective attention to details, ReadAgent achieves both computational efficiency and improved comprehension, demonstrating that mimicking human cognitive processes can significantly enhance AI text processing capabilities.\n\nMemoryBank The MemoryBank system enhances LLMs with long-term memory through a sophisticated three-part pipeline. At its core, the Memory Storage component warehouses detailed conversation logs, hierarchical event summaries, and evolving user personality profiles. When a new interaction occurs, the Memory Retrieval mechanism employs a dual-tower dense retrieval model to extract contextually relevant past information. The Memory Updating component, provides a human-like forgetting mechanism where memories strengthen when recalled and naturally decay over time if unused. This comprehensive approach enables AI companions to recall pertinent information, maintain contextual awareness across extended interactions, and develop increasingly accurate user portraits, resulting in more personalized and natural long-term conversations.\n\nMemGPT The MemGPT system introduces an operating system-inspired approach to overcome the context window limitations inherent in LLMs. At its core, MemGPT employs a sophisticated memory management pipeline consisting of three key components: a hierarchical memory system, self-directed memory operations, and an event-based control flow mechanism. The system divides available memory into 'main context' (analogous to RAM in traditional operating systems) and 'external context' (analogous to disk storage). The main context-which is bound by the LLM's context window-contains system instructions, recent conversation history, and working memory that can be modified by the model. The external context stores unlimited information outside the model's immediate context window, including complete conversation histories and archival data. When the LLM needs information not present in main context, it can initiate function calls to search, retrieve, or modify content across these memory tiers, effectively 'paging' relevant information in and out of its limited context window. This OS-inspired architecture enables MemGPT to maintain conversational coherence over extended interactions, manage documents that exceed standard context limits, and perform multi-hop information retrieval tasks-all while operating with fixed-context\n\nmodels. The system's ability to intelligently manage its own memory resources provides the illusion of infinite context, significantly extending what's possible with current LLM technology.\n\nA-Mem The A-Mem model introduces an agentic memory system designed for LLM agents. This system dynamically structures and evolves memories through interconnected notes. Each note captures interactions enriched with structured attributes like keywords, contextual descriptions, and tags generated by the LLM. Upon creating a new memory, A-MEM uses semantic embeddings to retrieve relevant existing notes, then employs an LLM-driven approach to establish meaningful links based on similarities and shared attributes. Crucially, the memory evolution mechanism updates existing notes dynamically, refining their contextual information and attributes whenever new relevant memories are integrated. Thus, memory structure continually evolves, allowing richer and contextually deeper connections among memories. Retrieval from memory is conducted through semantic similarity, providing relevant historical context during agent interactions\nOutput:",
      "name": null
    },
    {
      "role": "assistant",
      "content": "```json\n{\n    \"meta\": {\n        \"poster_title\": \"MemO: Building Production-Ready AI Agents with Scalable Long-Term Memory\",\n        \"authors\": \"Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav\",\n        \"affiliations\": \"research@mem0.ai\"\n    },\n    \"sections\": [\n        {\n            \"title\": \"Poster Title\",\n            \"content\": \"MemO: Building Production-Ready AI Agents with Scalable Long-Term Memory\\n\\nPrateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav\\n\\nresearch@mem0.ai\\n\\nCode: https://mem0.ai/research\"\n        },\n        {\n            \"title\": \"Abstract\",\n            \"content\": \"Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on the LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and $k$-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results demonstrate that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves $26 \\\\%$ relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around $2 \\\\%$ higher overall score than the base Mem0 configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to the full-context approach. In particular, Mem0 attains a $91 \\\\%$ lower p95 latency and saves more than $90 \\\\%$ token cost, thereby offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight the critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.\"\n        },\n        {\n            \"title\": \"Introduction\",\n            \"content\": \"Human memory is a foundation of intelligence—it shapes our identity, guides decision-making, and enables us to learn, adapt, and form meaningful relationships. AI agents, powered by large language models (LLMs), have made remarkable progress in generating fluent, contextually appropriate responses. However, these systems are fundamentally limited by their reliance on fixed context windows, which severely restrict their ability to maintain coherence over extended interactions. This limitation stems from LLMs' lack of persistent memory mechanisms that can extend beyond their finite context windows. The absence of persistent memory creates a fundamental disconnect in human-AI interaction. Without memory, AI agents forget user preferences, repeat questions, and contradict previously established facts. Figure 1 illustrates this: without persistent memory, a system forgets user dietary preferences; with memory, it maintains them. Memory mechanisms dramatically enhance agent performance in interactive environments. Unlike humans, LLMs effectively \\\"reset\\\" once information falls outside their context window. Even as models like OpenAI's GPT-4 (128K tokens), o1 (200K context), Anthropic's Claude 3.7 Sonnet (200K tokens), and Google's Gemini (at least 10M tokens) push context length boundaries, these improvements merely delay the fundamental limitation. Practically, even extended contexts are insufficient because conversation history often exceeds limits, and real-world conversations lack thematic continuity, burying critical information. Simply presenting longer contexts doesn't ensure effective retrieval due to attention degradation. To address these challenges, AI agents need memory systems that selectively store, consolidate, and retrieve important information. In this paper, we address AI systems' inability to maintain coherent reasoning across extended conversations. We introduce MemO, a novel memory architecture, and Memo${}^{\\\\text{g}}$, its graph-based enhancement. Our results show superior performance over existing systems with significantly lower computational resources.\"\n        },\n        {\n            \"title\": \"MemO System\",\n            \"content\": \"Our architecture follows an incremental processing paradigm, operating seamlessly within ongoing conversations. As illustrated in Figure 2, the pipeline has two phases: extraction and update.\\n\\nThe extraction phase initiates upon ingestion of a new message pair $(m_{t-1}, m_{t})$. Context is established using: (1) a conversation summary $S$ from the database, encapsulating the entire conversation history (refreshed asynchronously), and (2) a sequence of recent messages $\\\\{m_{t-m}, \\\\ldots, m_{t-2}\\\\}$ ($m$ is a hyperparameter). This dual contextual information, with the new message pair, forms a prompt $P=(S,\\\\{m_{t-m}, \\\\ldots, m_{t-2}\\\\}, m_{t-1}, m_{t})$ for an LLM-based extraction function $\\\\phi$. This function $\\\\phi(P)$ extracts salient memories $\\\\Omega=\\\\{\\\\omega_{1}, \\\\ldots, \\\\omega_{n}\\\\}$ from the new exchange.\\n\\nFollowing extraction, the update phase evaluates each candidate fact $\\\\omega_{i} \\\\in \\\\Omega$ against existing memories. For each fact, the system retrieves the top $s$ semantically similar memories using vector embeddings. These, along with the candidate fact, are presented to an LLM via a 'tool call'. The LLM determines one of four operations: ADD (new memory), UPDATE (augment existing memory), DELETE (remove contradicted memory), or NOOP (no change). This maintains knowledge base coherence.\\n\\nWe configured $m=10$ previous messages and $s=10$ similar memories. GPT-40-mini was used for LLM operations. The vector database uses dense embeddings for similarity search.\"\n        },\n        {\n            \"title\": \"MemO-g System\",\n            \"content\": \"The $\\\\text{MemO}^{g}$ pipeline (Figure 3) implements a graph-based memory approach. Memories are represented as a directed labeled graph $G=(V, E, L)$, where nodes $V$ are entities (e.g., Alice, San_Francisco), edges $E$ are relationships (e.g., LIVES_IN), and labels $L$ assign semantic types (e.g., Person, City). Each entity node includes type, embedding vector, and metadata (timestamp). Relationships are triplets $(v_{s}, r, v_{d})$.\\n\\nThe extraction process uses a two-stage LLM pipeline. First, an entity extractor identifies entities and types from input text—people, locations, objects, concepts, events, attributes. Next, a relationship generator derives connections, creating triplets that capture semantic structure by analyzing linguistic patterns and context.\\n\\nFor storage and updates, $\\\\text{MemO}^{g}$ computes embeddings for new relationship entities, searching for existing similar nodes (threshold 't'). It may create nodes or use existing ones. A conflict detection mechanism identifies conflicting relationships when new information arrives; an LLM-based resolver marks obsolete relationships as invalid (not removed) for temporal reasoning.\\n\\nMemory retrieval in $\\\\text{MemO}^{g}$ is dual-approach: (1) Entity-centric: identifies key query entities, finds corresponding graph nodes via semantic similarity, explores relationships to build a subgraph. (2) Semantic triplet: encodes the entire query as a dense vector, matches against textual encodings of all relationship triplets, returning relevant, ranked triplets.\\n\\nImplementation uses Neo4j as the graph database. LLM extractors and the update module leverage GPT-4o-mini with function calling.\"\n        },\n        {\n            \"title\": \"Evaluation Setup\",\n            \"content\": \"**Dataset**: The LOCOMO dataset is used, comprising 10 extended conversations (avg. 600 dialogues, 26,000 tokens each). Each conversation has ~200 questions categorized as single-hop, multi-hop, temporal, and open-domain.\\n**Evaluation Metrics**:\\n(1) Performance Metrics: F1 Score ($\\\\text{F}_{1}$), BLEU-1 ($\\\\text{B}_{1}$), and LLM-as-a-Judge (J) to assess response quality, factual accuracy, relevance, and completeness. J evaluations are averaged over 10 runs.\\n(2) Deployment Metrics: Token Consumption (tokens retrieved as context) and Latency (search latency and total response generation time).\\n**Baselines**: Six categories for comparison:\\n1. Established LOCOMO Benchmarks: LoCoMo, ReadAgent, MemoryBank, MemGPT, A-Mem.\\n2. Open-Source Memory Solutions: LangMem (Hot Path).\\n3. Retrieval-Augmented Generation (RAG): Various chunk sizes (128-8192 tokens) and top $k \\\\in \\\\{1,2\\\\}$ chunks retrieved.\\n4. Full-Context Processing: Entire conversation history as LLM input.\\n5. Proprietary Models: OpenAI's memory feature (gpt-4o-mini).\\n6. Memory Providers: Zep, a memory management platform.\"\n        },\n        {\n            \"title\": \"Performance Comparison\",\n            \"content\": \"Table 1 shows $\\\\text{F}_{1}$, $\\\\text{B}_{1}$, and J scores for Mem0 and $\\\\text{Mem0}^{g}$ against baselines. Our models generally set new state-of-the-art marks.\\n**Single-Hop Questions**: Mem0 achieves the strongest results ($\\\\text{F}_{1}=38.72, \\\\text{B}_{1}=27.13, \\\\text{J}=67.13$). $\\\\text{Mem0}^{g}$ performs similarly.\\n**Multi-Hop Questions**: MemO outperforms others ($\\\\text{F}_{1}=28.64, \\\\text{J}=51.15$). Graph memory in $\\\\text{Mem0}^{g}$ did not yield gains here, suggesting dense natural language memory is effective for these tasks.\\n**Open-Domain Performance**: Zep slightly leads ($\\\\text{J}=76.60$). $\\\\text{Mem0}^{g}$ is a strong runner-up ($\\\\text{J}=75.71$), followed by MemO ($\\\\text{J}=72.93$).\\n**Temporal Reasoning Performance**: $\\\\text{Mem0}^{g}$ excels ($\\\\text{F}_{1}=51.55, \\\\text{J}=58.13$), highlighting the benefit of structured relational representations. MemO also performs well ($\\\\text{J}=55.51$).\\n**Cross-Category Analysis**: MemO and $\\\\text{Mem0}^{g}$ consistently show superior performance. MemO's dense memory is efficient for simpler queries. $\\\\text{Mem0}^{g}$'s graph structures aid nuanced relational context, especially for temporal and open-domain tasks. Our methods achieve 5%, 11%, and 7% relative J score improvements in single-hop, temporal, and multi-hop categories, respectively, over prior bests.\"\n        },\n        {\n            \"title\": \"RAG & Full-Context\",\n            \"content\": \"Table 2 (Overall J column) shows Mem0 and $\\\\text{Mem0}^{g}$ outperform all RAG configurations. The best RAG achieves ~61% J, while Mem0 reaches 67% (~10% relative improvement) and $\\\\text{Mem0}^{g}$ over 68% (~12% relative gain). This highlights salient fact capture over large text chunk retrieval.\\n\\nA full-context method (using ~26,000 tokens) achieves the highest J score (~73%) but incurs very high p95 latency (~17 seconds). Mem0 and $\\\\text{Mem0}^{g}$ offer a practical trade-off: Mem0 has p95 latency of ~1.44s (92% reduction), and $\\\\text{Mem0}^{g}$ ~2.6s (85% reduction), maintaining near-competitive quality. As conversation length increases, full-context overhead grows, while memory-focused approaches like Mem0 and $\\\\text{Mem0}^{g}$ maintain consistent performance, making them more viable for production.\"\n        },\n        {\n            \"title\": \"Latency & Overhead\",\n            \"content\": \"**Latency Analysis (Table 2, Figure 4)**: Mem0 shows the lowest search latency (p50: 0.148s, p95: 0.200s) due to its selective memory retrieval. It also has the lowest total median latency (0.708s) and contained p95 (1.440s), ideal for interactive AI. $\\\\text{Mem0}^{g}$ has moderate latency costs (search p50: 0.476s; total p50: 1.091s) but achieves very high J scores. In contrast, full-context has p95 total latency of 17.117s. Mem0 achieves $91 \\\\%$ lower p95 latency and saves over $90 \\\\%$ token cost compared to full-context.\\n\\n**Memory System Overhead**: MemO averages ~7k tokens per conversation. $\\\\text{Mem0}^{g}$ averages ~14k tokens. Zep consumes over 600k tokens due to redundant storage of summaries and facts. The raw conversation context is ~26k tokens. Zep exhibited significant operational delays in memory availability. MemO's graph construction is under a minute, allowing immediate memory use. Our architectures are more memory-efficient and operationally responsive.\"\n        },\n        {\n            \"title\": \"Conclusion\",\n            \"content\": \"We introduced MemO and $\\\\text{MemO}^{g}$, complementary memory architectures addressing LLM context window limitations. MemO excels in single-hop and multi-hop reasoning, while $\\\\text{MemO}^{g}$'s graph extensions boost temporal and open-domain tasks. On LOCOMO, our methods show significant improvements (e.g., 5-11% relative J score gains in various categories) and reduce p95 latency by over $91 \\\\%$ versus full-context, balancing precision and responsiveness. MemO's dense memory is efficient for rapid retrieval; $\\\\text{MemO}^{g}$'s structured graph offers nuanced relational clarity.\\n\\n**Future Work**: Optimizing $\\\\text{MemO}^{g}$ graph operations, exploring hierarchical memory, developing advanced consolidation mechanisms, and extending to new domains like procedural reasoning and multimodal interactions. This work advances conversational AI towards systems with coherent, context-rich, long-term interactions.\"\n        }\n    ]\n}\n```",
      "name": null
    }
  ]
}